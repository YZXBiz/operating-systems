# üîß Chapter 3: Redundant Arrays of Inexpensive Disks (RAID)

## Table of Contents

1. [Introduction](#1-introduction)
2. [The Core Challenge](#2-the-core-challenge)
3. [RAID Fundamentals](#3-raid-fundamentals)
   - 3.1. [What is RAID?](#31-what-is-raid)
   - 3.2. [Interface and Internals](#32-interface-and-internals)
   - 3.3. [Fault Model: Fail-Stop](#33-fault-model-fail-stop)
   - 3.4. [Evaluation Criteria](#34-evaluation-criteria)
4. [RAID Level 0: Striping](#4-raid-level-0-striping)
   - 4.1. [How Striping Works](#41-how-striping-works)
   - 4.2. [Chunk Sizes and Layout](#42-chunk-sizes-and-layout)
   - 4.3. [The Mapping Problem](#43-the-mapping-problem)
   - 4.4. [Performance Analysis](#44-performance-analysis)
5. [RAID Level 1: Mirroring](#5-raid-level-1-mirroring)
   - 5.1. [How Mirroring Works](#51-how-mirroring-works)
   - 5.2. [The Consistent-Update Problem](#52-the-consistent-update-problem)
   - 5.3. [Performance Analysis](#53-performance-analysis)
6. [RAID Level 4: Parity-Based Redundancy](#6-raid-level-4-parity-based-redundancy)
   - 6.1. [Understanding XOR Parity](#61-understanding-xor-parity)
   - 6.2. [Parity Calculation Methods](#62-parity-calculation-methods)
   - 6.3. [The Small-Write Problem](#63-the-small-write-problem)
   - 6.4. [Performance Analysis](#64-performance-analysis)
7. [RAID Level 5: Rotating Parity](#7-raid-level-5-rotating-parity)
   - 7.1. [How Rotating Parity Works](#71-how-rotating-parity-works)
   - 7.2. [Performance Analysis](#72-performance-analysis)
8. [RAID Comparison Summary](#8-raid-comparison-summary)
9. [Additional RAID Topics](#9-additional-raid-topics)
10. [Key Insights and Tradeoffs](#10-key-insights-and-tradeoffs)

---

## 1. Introduction

üéØ **The Challenge**: Single disks are slow, limited in capacity, and vulnerable to failure. How do we build storage that's fast, large, AND reliable?

**In plain English:** Imagine using multiple cheap disks together as if they were one super-disk that's faster, bigger, and won't lose your data if one disk breaks.

**In technical terms:** RAID (Redundant Array of Inexpensive Disks) combines multiple physical disks into a single logical storage unit that appears as one disk to the system above it.

**Why it matters:** RAID provides performance, capacity, and reliability improvements transparently‚Äîmeaning your operating system and applications don't need to change at all to benefit from these improvements.

> **üí° Insight**
>
> The power of RAID lies in **transparency**. You can replace a single disk with a RAID array without changing a single line of software. This enabled RAID to succeed where many other storage innovations failed‚Äîno deployment barriers.

### Historical Context

RAID was introduced in the late 1980s by researchers at UC Berkeley (Professors David Patterson and Randy Katz, with student Garth Gibson). Multiple research groups simultaneously arrived at the idea of using multiple disks to build better storage systems.

---

## 2. The Core Challenge

### üéØ The Crux: How to Make a Large, Fast, Reliable Disk

When we use disks, we face three fundamental wishes:

1. **Speed**: I/O operations are slow and bottleneck the entire system
2. **Capacity**: We need to store more and more data
3. **Reliability**: When a disk fails, all valuable data is lost

The challenge is achieving all three simultaneously. Traditional single-disk systems force you to choose.

---

## 3. RAID Fundamentals

### 3.1. What is RAID?

**External View**: A RAID looks like a single disk‚Äîa linear array of blocks you can read or write.

**Internal Reality**: A RAID is a complex system containing:
- Multiple physical disks
- Memory (both volatile DRAM and non-volatile)
- One or more processors running specialized firmware
- Sometimes specialized logic for parity calculations

> **üí° Insight**
>
> A hardware RAID is essentially a specialized computer system dedicated to managing a group of disks. It has processors, memory, and disks‚Äîjust like a computer‚Äîbut runs firmware designed specifically for storage management.

### 3.2. Interface and Internals

üîÑ **How It Works**:

```
File System Request:
Write logical block 5
        ‚Üì
RAID Controller:
Calculate physical locations
        ‚Üì
Physical Operations:
Write to disk 2, block 10
Write to disk 3, block 10 (mirror copy)
```

The RAID internally translates each **logical I/O** (what the file system requests) into one or more **physical I/Os** (actual disk operations). The exact translation depends on the RAID level.

**Example**: In a mirrored RAID:
- 1 logical write ‚Üí 2 physical writes (one to each mirror)
- 1 logical read ‚Üí 1 physical read (from either mirror)

### 3.3. Fault Model: Fail-Stop

üîí **The Fail-Stop Model**:

A disk can be in exactly two states:
- **Working**: All blocks can be read or written
- **Failed**: The disk is permanently lost

**Key Assumptions**:
1. Failures are immediately detectable (no silent failures)
2. Entire disk fails (not individual blocks)
3. No disk corruption issues (simplified model)

> **‚ö†Ô∏è Note**
>
> This is a simplified fault model. Real systems face more complex failures like latent sector errors (single bad blocks) and silent data corruption. These are addressed in more advanced RAID designs.

### 3.4. Evaluation Criteria

We evaluate RAID designs along three axes:

#### üìä 1. Capacity

**Question**: Given N disks with B blocks each, how much usable capacity do we get?

- No redundancy: `N √ó B` blocks
- Mirroring (2 copies): `(N √ó B) / 2` blocks
- Parity-based: Between these extremes

#### üõ°Ô∏è 2. Reliability

**Question**: How many disk failures can the system tolerate?

Different RAID levels tolerate different numbers of failures:
- RAID-0: 0 failures (no redundancy)
- RAID-1: 1 failure guaranteed
- RAID-5: 1 failure
- RAID-6: 2 failures

#### ‚ö° 3. Performance

**Question**: How fast can the RAID handle different workloads?

Performance depends heavily on the workload type:

**Sequential Workload**:
- Large contiguous chunks (e.g., 1 MB starting at block x)
- Common in file scanning, video streaming
- Disk operates efficiently: minimal seeking, mostly transferring

**Random Workload**:
- Small requests (e.g., 4 KB) at scattered locations
- Common in databases, transactional systems
- Disk operates inefficiently: mostly seeking, little transferring

**Performance Metrics**:
- `S`: Sequential throughput (MB/s) per disk
- `R`: Random throughput (MB/s) per disk
- Generally: `S >> R` (S is much greater than R)

**üìù Example Calculation**:

Disk specifications:
- Average seek time: 7 ms
- Average rotational delay: 3 ms
- Transfer rate: 50 MB/s

For **10 MB sequential transfer**:
- Seek: 7 ms
- Rotation: 3 ms
- Transfer: 10 MB √∑ 50 MB/s = 200 ms
- **Total**: 210 ms
- **S = 10 MB / 210 ms ‚âà 47.6 MB/s** (close to peak!)

For **10 KB random transfer**:
- Seek: 7 ms
- Rotation: 3 ms
- Transfer: 10 KB √∑ 50 MB/s ‚âà 0.2 ms
- **Total**: 10.2 ms
- **R = 10 KB / 10.2 ms ‚âà 0.98 MB/s** (very slow!)

The ratio S/R ‚âà 50, showing how workload dramatically affects performance.

---

## 4. RAID Level 0: Striping

### 4.1. How Striping Works

**In plain English:** Spread your data across all disks in a round-robin fashion, like dealing cards to players around a table.

**In technical terms:** Distribute blocks sequentially across disks to maximize parallelism when accessing contiguous data.

**Simple Layout** (4-disk array):

```
Disk 0    Disk 1    Disk 2    Disk 3
  0         1         2         3      ‚Üê Stripe 0
  4         5         6         7      ‚Üê Stripe 1
  8         9        10        11      ‚Üê Stripe 2
 12        13        14        15      ‚Üê Stripe 3
```

A **stripe** is the set of blocks in the same row (e.g., blocks 0, 1, 2, 3).

### 4.2. Chunk Sizes and Layout

The **chunk size** determines how many blocks go on each disk before moving to the next.

**Small Chunk (1 block = 4 KB)**:
```
Disk 0    Disk 1    Disk 2    Disk 3
  0         1         2         3
  4         5         6         7
```

**Larger Chunk (2 blocks = 8 KB)**:
```
Disk 0    Disk 1    Disk 2    Disk 3
  0         2         4         6
  1         3         5         7
  8        10        12        14
  9        11        13        15
```

#### üé® Chunk Size Tradeoffs

**Small Chunks**:
- ‚úÖ More parallelism for single files (striped across many disks)
- ‚ùå Higher positioning time (seeking across multiple disks)

**Large Chunks**:
- ‚úÖ Lower positioning time (fewer disk switches)
- ‚ùå Less intra-file parallelism (file may fit on one disk)
- ‚úÖ Relies on multiple concurrent requests for throughput

> **üí° Insight**
>
> Choosing the "best" chunk size requires deep knowledge of your workload. Most arrays use 64 KB chunks as a reasonable compromise. For our analysis, we'll assume 4 KB (1 block) for simplicity.

### 4.3. The Mapping Problem

üîç **Challenge**: Given a logical block address, which physical disk and offset should we access?

For simple striping (chunk size = 1 block):

```
Disk   = A % number_of_disks
Offset = A / number_of_disks
```

**Example**: Find block 14 in a 4-disk array:
- Disk = 14 % 4 = **2** (third disk, zero-indexed)
- Offset = 14 / 4 = **3** (fourth block on that disk)

Looking at our layout, block 14 is indeed at disk 2, offset 3. ‚úì

### 4.4. Performance Analysis

#### üìä Capacity
- **Formula**: `N √ó B` blocks
- **Result**: Perfect! Full capacity of all disks

#### üõ°Ô∏è Reliability
- **Tolerance**: 0 disk failures
- **Result**: Terrible! Any single disk failure loses all data

#### ‚ö° Performance

**Sequential Read/Write**:
- All disks work in parallel
- **Throughput**: `N √ó S` MB/s
- **Result**: Excellent! Maximum possible bandwidth

**Random Read/Write**:
- All disks handle different requests
- **Throughput**: `N √ó R` MB/s
- **Result**: Excellent! Maximum possible bandwidth

**Latency**:
- Single request goes to one disk
- **Latency**: Same as single disk

> **üí° Insight**
>
> RAID-0 serves as the **performance upper bound** for all RAID levels. Every other level trades some of this performance for reliability or capacity efficiency.

---

## 5. RAID Level 1: Mirroring

### 5.1. How Mirroring Works

**In plain English:** Keep two identical copies of everything on separate disks, like keeping a backup notebook in case you lose the first one.

**In technical terms:** Maintain multiple copies of each block on different disks, typically with striping across mirror pairs.

**RAID-10 Layout** (4 disks, 2-way mirroring):

```
Disk 0    Disk 1    Disk 2    Disk 3
  0         0         1         1      ‚Üê Block 0 mirrored, Block 1 mirrored
  2         2         3         3      ‚Üê Block 2 mirrored, Block 3 mirrored
  4         4         5         5
  6         6         7         7
```

- Disks 0 and 1: Mirror pair
- Disks 2 and 3: Mirror pair
- Data striped across mirror pairs

**Read Operations**:
- Can read from **either** copy (flexibility!)
- Example: Read block 5 from disk 2 OR disk 3

**Write Operations**:
- Must write to **both** copies (mandatory!)
- Writes happen in parallel
- Example: Write block 5 to disk 2 AND disk 3 simultaneously

### 5.2. The Consistent-Update Problem

‚ö†Ô∏è **The Problem**: What happens if the system crashes between writing the first and second copy?

**Scenario**:
```
1. Write to block 1 requested
2. RAID writes to disk 0 (completes)
3. ‚ö° POWER FAILURE ‚ö°
4. RAID never writes to disk 1
```

**Result**: The two copies are now **inconsistent**!
- Disk 0: New version
- Disk 1: Old version

**What we want**: **Atomic updates**‚Äîeither both copies update or neither does.

#### üîß Solution: Write-Ahead Logging

**How it works**:
1. **Log the intent**: Write "about to update disks 0 and 1 with data X" to a log
2. **Perform the update**: Write to both disks
3. **Mark complete**: Note in log that update finished

After a crash:
- **Recovery procedure** replays all pending logged transactions
- Ensures all mirrors stay in sync

**Implementation Detail**:
- Logging to disk every write is too slow
- Most RAID hardware uses **battery-backed non-volatile RAM**
- Fast logging without disk overhead

> **üí° Insight**
>
> The consistent-update problem appears in any RAID that updates multiple locations for one logical write. The solution‚Äîwrite-ahead logging‚Äîis a pattern used throughout computer systems, from databases to file systems to RAIDs.

### 5.3. Performance Analysis

#### üìä Capacity
- **Formula**: `(N √ó B) / 2` blocks (with 2-way mirroring)
- **Result**: Expensive! Only half the total capacity is usable

#### üõ°Ô∏è Reliability
- **Guaranteed**: 1 disk failure
- **Lucky case**: Up to N/2 failures (if the right disks fail)
- **Example**: In the layout above, disk 0 and disk 2 can both fail without data loss

> **‚ö†Ô∏è Note**
>
> In practice, we design for the guaranteed case (1 failure), not the lucky case.

#### ‚ö° Performance

**Single Request Latency**:

**Read**: Same as single disk
- Route to one copy
- No overhead

**Write**: Slightly higher than single disk
- Must write to both copies in parallel
- Completes when both finish
- Suffers **worst-case** seek/rotation of the two disks

**Steady-State Throughput**:

**Sequential Read**: `(N / 2) √ó S` MB/s

Why not full bandwidth? Let's trace through an example.

Reading blocks 0-7 sequentially:
- Block 0 ‚Üí Disk 0
- Block 1 ‚Üí Disk 2
- Block 2 ‚Üí Disk 1
- Block 3 ‚Üí Disk 3
- Block 4 ‚Üí Disk 0 (skipping block 2!)
- Block 5 ‚Üí Disk 2 (skipping block 3!)
- ...

Each disk gets every **other** block in its logical sequence. While rotating over the skipped block, it's not delivering data. Result: Each disk delivers half its bandwidth.

**Sequential Write**: `(N / 2) √ó S` MB/s
- Each logical write ‚Üí 2 physical writes
- All disks busy, but effective bandwidth is halved

**Random Read**: `N √ó R` MB/s ‚≠ê **Best case!**
- Distribute reads across all disks
- Can read from either copy
- Full parallelism achieved

**Random Write**: `(N / 2) √ó R` MB/s
- Each logical write ‚Üí 2 physical writes (parallel)
- All disks busy, but effective bandwidth is halved

> **üí° Insight**
>
> RAID-1's random read performance is excellent because you can leverage both copies. Every other operation pays a 2√ó overhead for maintaining redundancy. This is the cost of reliability.

---

## 6. RAID Level 4: Parity-Based Redundancy

üéØ **Goal**: Achieve redundancy with less capacity overhead than mirroring.

**The Idea**: Instead of keeping full copies, store **parity information** that lets you reconstruct data if one disk fails.

### 6.1. Understanding XOR Parity

**In plain English:** Keep a special "summary" that tells you what's missing if one piece disappears, like how you can figure out a missing number when you know the sum.

**In technical terms:** Use XOR (exclusive OR) to compute redundant information that enables reconstruction of any single lost block.

#### How XOR Works

**XOR Truth Table** (for a single bit):
```
0 XOR 0 = 0
0 XOR 1 = 1
1 XOR 0 = 1
1 XOR 1 = 0
```

**Pattern**: XOR returns 1 if there's an **odd** number of 1's, and 0 if there's an **even** number.

**Invariant**: The number of 1's in any row (including parity) must be **even**.

#### Parity Example

**Computing Parity**:
```
C0   C1   C2   C3   P (Parity)
0    0    1    1    XOR(0,0,1,1) = 0   (two 1's = even)
0    1    0    0    XOR(0,1,0,0) = 1   (one 1  = odd)
```

**Reconstructing Lost Data**:

Suppose C2 in the first row is lost:
```
C0   C1   C2   C3   P
0    0    ?    1    0
```

XOR all known values (including parity):
```
C2 = XOR(C0, C1, C3, P)
   = XOR(0, 0, 1, 0)
   = 1 ‚úì
```

The missing value must be 1 to maintain an even number of 1's!

#### Block-Level Parity

For 4 KB blocks, apply XOR **bitwise** across all blocks:

```
Block 0:  0010...
Block 1:  1001...
Block 2:  1100...
Block 3:  1001...
         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Parity:   1110...  (XOR of each bit position)
```

**RAID-4 Layout** (5 disks):

```
Disk 0   Disk 1   Disk 2   Disk 3   Disk 4 (Parity)
  0        1        2        3        P0
  4        5        6        7        P1
  8        9       10       11        P2
 12       13       14       15        P3
```

- **P0** = XOR(0, 1, 2, 3)
- **P1** = XOR(4, 5, 6, 7)
- **P2** = XOR(8, 9, 10, 11)
- **P3** = XOR(12, 13, 14, 15)

### 6.2. Parity Calculation Methods

When writing to RAID-4, we must update both data and parity. Two methods exist:

#### Method 1: Additive Parity (Full-Stripe Write)

**When**: Writing all blocks in a stripe (e.g., blocks 0, 1, 2, 3)

**Process**:
1. Calculate new parity: `P_new = XOR(0, 1, 2, 3)`
2. Write all data blocks in parallel
3. Write parity block

**Performance**: Very efficient for sequential writes!

**Sequential Write Throughput**: `(N - 1) √ó S` MB/s

#### Method 2: Subtractive Parity (Partial-Stripe Write)

**When**: Writing a single block (e.g., only block 1)

**Problem**: Can't use additive method (would need to read blocks 0, 2, 3‚Äîinefficient as array scales!)

**Process**:
1. **Read** old data block: `C_old`
2. **Read** old parity block: `P_old`
3. **Compute** new parity: `P_new = XOR(C_old, C_new, P_old)`
4. **Write** new data block: `C_new`
5. **Write** new parity block: `P_new`

**Formula**:
```
P_new = C_old ‚äï C_new ‚äï P_old
```

**Why it works**:
- If `C_new = C_old`: No change, so `P_new = P_old`
- If `C_new ‚â† C_old`: Flip the parity bit

**I/O Count**: 4 physical I/Os per logical write
- 2 reads (old data, old parity)
- 2 writes (new data, new parity)

> **üí° Insight**
>
> The choice between additive and subtractive parity depends on how many blocks you're writing. Writing a full stripe? Use additive. Writing one or two blocks? Use subtractive. Most systems use subtractive for typical workloads.

### 6.3. The Small-Write Problem

‚ö†Ô∏è **The Bottleneck**: The parity disk becomes a bottleneck for random writes!

**Scenario**: Two concurrent writes to blocks 4 and 13

```
Disk 0   Disk 1   Disk 2   Disk 3   Disk 4 (Parity)
  0        1        2        3        P0
  4*       5        6        7        P1*     ‚Üê Write to 4 needs P1
  8        9       10       11        P2
 12       13*      14       15        P3*     ‚Üê Write to 13 needs P3
```

**The Problem**:
- Data writes to disks 0 and 1: ‚úÖ Can happen in parallel
- Parity reads/writes to disk 4: ‚ùå Must be serialized!

Both writes need to:
1. Read old parity from disk 4
2. Write new parity to disk 4

**Result**: All writes get **serialized** through the parity disk.

### 6.4. Performance Analysis

#### üìä Capacity
- **Formula**: `(N - 1) √ó B` blocks
- **Result**: Good! Only one disk used for parity

#### üõ°Ô∏è Reliability
- **Tolerance**: 1 disk failure
- **Result**: Good! Can reconstruct from parity

#### ‚ö° Performance

**Sequential Read**: `(N - 1) √ó S` MB/s
- Use all data disks (not parity disk)
- Excellent performance

**Sequential Write**: `(N - 1) √ó S` MB/s
- Use full-stripe writes
- Excellent performance

**Random Read**: `(N - 1) √ó R` MB/s
- Distribute across data disks
- Good performance

**Random Write**: `R / 2` MB/s ‚ùå **Terrible!**
- Parity disk bottleneck
- Parity disk does 2 I/Os per logical I/O (1 read, 1 write)
- Performance doesn't improve with more disks!

**Latency**:
- **Read**: Same as single disk
- **Write**: ~2√ó single disk (read old data/parity, write new data/parity)

> **‚ö†Ô∏è Critical Problem**
>
> RAID-4's random write performance is abysmal. The parity disk bottleneck makes it unsuitable for workloads with many small, random writes‚Äîwhich is common in databases and transactional systems.

---

## 7. RAID Level 5: Rotating Parity

üéØ **Goal**: Solve RAID-4's parity bottleneck by distributing parity across all disks.

### 7.1. How Rotating Parity Works

**The Innovation**: Rotate which disk holds parity for each stripe.

**RAID-5 Layout** (5 disks):

```
Disk 0   Disk 1   Disk 2   Disk 3   Disk 4
  0        1        2        3        P0     ‚Üê Parity on disk 4
  5        6        7        P1       4      ‚Üê Parity on disk 3
 10       11       P2       8        9      ‚Üê Parity on disk 2
 15       P3      12       13       14      ‚Üê Parity on disk 1
 P4      16       17       18       19      ‚Üê Parity on disk 0
```

**Pattern**: Each stripe's parity is on a different disk.

**Why this helps**: Concurrent writes to different stripes can now proceed in parallel!

**Example**: Write to block 1 and block 10
```
Block 1  ‚Üí Disk 1 (data) + Disk 4 (parity P0)
Block 10 ‚Üí Disk 0 (data) + Disk 2 (parity P2)
```

No disk conflict! ‚úì Both writes can happen simultaneously.

### 7.2. Performance Analysis

Most characteristics are identical to RAID-4:

#### üìä Capacity
- **Formula**: `(N - 1) √ó B` blocks
- Same as RAID-4

#### üõ°Ô∏è Reliability
- **Tolerance**: 1 disk failure
- Same as RAID-4

#### ‚ö° Performance

**Sequential Read**: `(N - 1) √ó S` MB/s
- Same as RAID-4

**Sequential Write**: `(N - 1) √ó S` MB/s
- Same as RAID-4 (full-stripe writes)

**Random Read**: `N √ó R` MB/s ‚≠ê **Better!**
- Can now use all disks (including the rotating parity disk)

**Random Write**: `(N / 4) √ó R` MB/s ‚≠ê **Much better!**
- Parity distributed, so writes can be parallel
- Still 4 I/Os per logical write (factor of 4)
- But now scales with number of disks!

**Latency**:
- Same as RAID-4

> **üí° Insight**
>
> RAID-5 solves the small-write problem through clever layout, not by reducing I/O count. Each write still does 4 physical I/Os, but now different writes can use different disks simultaneously. This is **parallelism through distribution**.

#### üéì Why the Factor of 4?

Each logical write generates:
1. Read old data (1 I/O)
2. Read old parity (1 I/O)
3. Write new data (1 I/O)
4. Write new parity (1 I/O)

Total: **4 physical I/Os per logical I/O**

With many random writes distributed evenly across disks:
- Total I/Os available: `N √ó R`
- Each logical write consumes 4 physical I/Os
- **Effective throughput**: `(N / 4) √ó R` MB/s

---

## 8. RAID Comparison Summary

### üìä Complete Comparison Table

| Metric | RAID-0 | RAID-1 | RAID-4 | RAID-5 |
|--------|--------|--------|--------|--------|
| **Capacity** | `N √ó B` | `(N √ó B) / 2` | `(N - 1) √ó B` | `(N - 1) √ó B` |
| **Reliability** | 0 failures | 1 failure (up to N/2 lucky) | 1 failure | 1 failure |
| **Sequential Read** | `N √ó S` | `(N / 2) √ó S` | `(N - 1) √ó S` | `(N - 1) √ó S` |
| **Sequential Write** | `N √ó S` | `(N / 2) √ó S` | `(N - 1) √ó S` | `(N - 1) √ó S` |
| **Random Read** | `N √ó R` | `N √ó R` | `(N - 1) √ó R` | `N √ó R` |
| **Random Write** | `N √ó R` | `(N / 2) √ó R` | `R / 2` | `(N / 4) √ó R` |
| **Read Latency** | T | T | T | T |
| **Write Latency** | T | T | 2T | 2T |

Where:
- `N`: Number of disks
- `B`: Blocks per disk
- `S`: Sequential throughput per disk (MB/s)
- `R`: Random throughput per disk (MB/s)
- `T`: Single disk latency

### üéØ When to Use Each RAID Level

**RAID-0 (Striping)**:
- ‚úÖ **Use when**: Performance is critical, no reliability needed
- ‚ùå **Avoid when**: Data loss is unacceptable
- üéØ **Best for**: Temporary data, caches, scratch space

**RAID-1 (Mirroring)**:
- ‚úÖ **Use when**: Random I/O performance + reliability needed
- ‚ùå **Avoid when**: Capacity is constrained
- üéØ **Best for**: Databases, transaction logs, high-IOPS workloads

**RAID-4 (Parity)**:
- ‚úÖ **Use when**: Only large sequential writes expected
- ‚ùå **Avoid when**: Any random writes expected
- üéØ **Best for**: Specialized systems, mostly replaced by RAID-5

**RAID-5 (Rotating Parity)**:
- ‚úÖ **Use when**: Capacity + reliability + sequential I/O needed
- ‚ùå **Avoid when**: High random write workload
- üéØ **Best for**: File servers, backup systems, archival storage

> **üí° Insight**
>
> RAID-5 has almost completely replaced RAID-4 in practice because it's identical except where it's better. RAID-4 only survives in niche cases where the workload is guaranteed to avoid the small-write problem.

---

## 9. Additional RAID Topics

### üîß Other RAID Levels

**RAID-2 and RAID-3**: From the original taxonomy, rarely used today

**RAID-6**: Tolerates **2 disk failures**
- Uses two parity schemes (P and Q)
- Critical for large arrays where rebuild time is long
- More common as disk sizes grow

### üîÑ Failure Handling

**Hot Spares**: Extra disks standing by to replace failed disks automatically

**Rebuild Process**:
1. Detect disk failure
2. Activate hot spare
3. Reconstruct data using parity/mirrors
4. Normal operation resumes

**Performance During Rebuild**: Degraded, as reconstruction competes with user I/O

### üêõ More Realistic Fault Models

**Latent Sector Errors**:
- Single blocks become unreadable
- Not entire disk failure
- Requires additional error detection/correction

**Silent Data Corruption**:
- Data changes without detection
- Requires checksums and scrubbing
- Covered in data integrity chapters

### üíª Software RAID

**Hardware RAID**: Dedicated controller, battery-backed RAM, specialized processors

**Software RAID**: Implemented in the OS kernel

**Tradeoffs**:
- Software RAID is cheaper
- Software RAID faces the consistent-update problem more severely
- Hardware RAID offloads work from CPU
- Software RAID is more flexible

---

## 10. Key Insights and Tradeoffs

### üß† The Fundamental Tradeoffs

**Performance vs. Reliability vs. Capacity**: Pick two

```
              Performance
                  /\
                 /  \
                /    \
               /      \
              /        \
             /  RAID-0  \
            /            \
           /              \
          /                \
         /                  \
        /                    \
    RAID-1 ............... RAID-5
  Reliability            Capacity
```

- **RAID-0**: Performance + Capacity, no reliability
- **RAID-1**: Performance + Reliability, low capacity
- **RAID-5**: Capacity + Reliability, reduced performance

### ‚ö° The 4 I/O Problem

Every parity-based RAID pays a **4√ó penalty** on random writes:
1. Read old data
2. Read old parity
3. Write new data
4. Write new parity

This is unavoidable with single-disk fault tolerance using parity.

### üéØ Transparency as a Superpower

RAID's success stems from being a **drop-in replacement**:
- No software changes required
- No application modifications
- Works with existing systems
- Easy deployment

> **üí° Final Insight**
>
> RAID demonstrates that **transparency** and **deployability** matter as much as technical superiority. Many technically excellent systems failed because they required too much change. RAID succeeded because it required none.

### üîç Choosing RAID Parameters

Selecting RAID configuration is **more art than science**:

**Consider**:
- Workload characteristics (sequential vs. random, read vs. write)
- Capacity requirements
- Reliability requirements
- Budget constraints
- Rebuild time acceptable
- Performance needs

**No universal best choice**‚Äîit depends entirely on your specific needs.

---

**Previous:** [Chapter 2: Hard Disk Drives](chapter2-hard-disk-drives.md) | **Next:** [Chapter 4: Files and Directories](chapter4-files-directories.md)
