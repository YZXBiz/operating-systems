# Chapter 6: Proportional Share Scheduling ğŸ°

_Understanding how to fairly divide CPU time using lottery tickets and deterministic strides_

---

## ğŸ“‹ Table of Contents

1. [ğŸ¯ Introduction](#1-introduction)
2. [ğŸ« Basic Concept: Tickets Represent Your Share](#2-basic-concept-tickets-represent-your-share)
   - 2.1. [How Lottery Scheduling Works](#21-how-lottery-scheduling-works)
   - 2.2. [The Power of Randomness](#22-the-power-of-randomness)
3. [ğŸ”§ Ticket Mechanisms](#3-ticket-mechanisms)
   - 3.1. [Ticket Currency](#31-ticket-currency)
   - 3.2. [Ticket Transfer](#32-ticket-transfer)
   - 3.3. [Ticket Inflation](#33-ticket-inflation)
4. [ğŸ’» Implementation](#4-implementation)
   - 4.1. [Data Structures](#41-data-structures)
   - 4.2. [Scheduling Algorithm](#42-scheduling-algorithm)
   - 4.3. [Optimization Strategies](#43-optimization-strategies)
5. [ğŸ“Š Fairness Analysis](#5-fairness-analysis)
   - 5.1. [Short-Term vs Long-Term Fairness](#51-short-term-vs-long-term-fairness)
   - 5.2. [Experimental Results](#52-experimental-results)
6. [ğŸ¯ Stride Scheduling](#6-stride-scheduling)
   - 6.1. [Deterministic Fair-Share](#61-deterministic-fair-share)
   - 6.2. [Stride vs Lottery Tradeoffs](#62-stride-vs-lottery-tradeoffs)
7. [ğŸ§ Linux Completely Fair Scheduler (CFS)](#7-linux-completely-fair-scheduler-cfs)
   - 7.1. [Virtual Runtime](#71-virtual-runtime)
   - 7.2. [Time Slice Calculation](#72-time-slice-calculation)
   - 7.3. [Process Weighting (Nice Values)](#73-process-weighting-nice-values)
   - 7.4. [Red-Black Trees](#74-red-black-trees)
   - 7.5. [Handling I/O and Sleeping Processes](#75-handling-io-and-sleeping-processes)
8. [âš ï¸ Challenges and Limitations](#8-challenges-and-limitations)
9. [ğŸ“ Summary](#9-summary)

---

## 1. ğŸ¯ Introduction

**In plain English:** Imagine you're running a restaurant ğŸ½ï¸ with one chef ğŸ‘¨â€ğŸ³ and multiple customer orders. Instead of trying to finish orders as fast as possible (like previous scheduling approaches), what if you wanted to give each customer a **fair share** of the chef's time based on how much they paid? A VIP customer who paid $100 gets more chef time than someone who paid $10. That's proportional-share scheduling.

**In technical terms:** Proportional-share schedulers guarantee that each job obtains a certain **percentage of CPU time**, rather than optimizing for turnaround time or response time. Instead of asking "which job finishes fastest?" they ask "does each job get its fair share?"

**Why it matters:** This approach is crucial for:
- ğŸ–¥ï¸ **Virtual machines** - Allocating CPU fairly across VMs
- â˜ï¸ **Cloud computing** - Giving customers the CPU they paid for
- ğŸ® **Game servers** - Ensuring each player gets equal processing
- ğŸ“± **Mobile devices** - Balancing foreground/background apps

> **ğŸ’¡ Insight**
>
> Proportional-share scheduling represents a **fundamental shift in goals**. Previous schedulers (FIFO, SJF, RR) focused on **performance metrics** (turnaround, response time). Proportional-share focuses on **fairness metrics** (each process gets its promised share). This distinction mirrors resource allocation everywhere: Should we optimize for speed or for fairness? Both matter, just in different contexts.

### ğŸ¯ The Core Challenge

**THE CRUX:** How can we design a scheduler to share the CPU in a proportional manner? What are the key mechanisms for doing so? How effective are they?

**Three approaches we'll explore:**

```
Lottery Scheduling          Stride Scheduling           CFS (Linux)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ² Probabilistic           âœ… Deterministic            âš–ï¸ Virtual Runtime
Random ticket draw         Precise stride tracking     Efficient tracking
Simple, lightweight        Accurate, predictable       Production-ready
```

---

## 2. ğŸ« Basic Concept: Tickets Represent Your Share

### 2.1. How Lottery Scheduling Works

**In plain English:** Think of CPU time as a raffle ğŸŸï¸. Every process gets tickets based on how much CPU it should receive. Every time slice (say, every 10ms), we draw a random winning ticket. Whoever owns that ticket runs for the next time slice. Processes with more tickets win more oftenâ€”**probabilistically** getting their fair share over time.

**Core principles:**

```
Ticket Distribution                    Probability
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Process A: 75 tickets    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   75% chance to win
Process B: 25 tickets    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   25% chance to win
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 100 tickets
```

**Step-by-step lottery example:**

```
Time Slice    Random Number    Winner      Schedule
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€
1             63 (A's range)   A           A runs
2             85 (B's range)   B           B runs
3             70 (A's range)   A           A runs
4             39 (A's range)   A           A runs
5             76 (B's range)   B           B runs
6             17 (A's range)   A           A runs
7             29 (A's range)   A           A runs
8             41 (A's range)   A           A runs

Ticket Ranges:
A holds tickets: 0-74   (75 tickets)
B holds tickets: 75-99  (25 tickets)
```

**Resulting execution pattern:**
```
A A B A A B A A A A A A A A A A B B B B
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Over 20 time slices:
    A ran 16 times (80%) - close to target 75%
    B ran 4 times (20%) - close to target 25%
```

> **ğŸ’¡ Insight**
>
> Notice how **fairness emerges over time** but isn't guaranteed in any short window. In the example, B got exactly 20% instead of 25%â€”but run this for thousands of time slices, and the percentages converge to the desired values. This is **probabilistic correctness**, similar to how randomized algorithms in data structures (skip lists, hash tables) give expected-case guarantees.

### 2.2. ğŸ² The Power of Randomness

**Why use randomness instead of deterministic algorithms?**

**In plain English:** Imagine trying to fairly split dinner duty among roommates ğŸ . You could maintain a complex schedule tracking exactly who did what when. Or you could draw straws randomlyâ€”simpler, and over time it evens out. Randomness trades short-term precision for long-term simplicity.

**Three key advantages of randomness:**

#### 1ï¸âƒ£ Avoids Corner Cases

```
Deterministic LRU                Randomized Replacement
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Cyclic access pattern:           Random selection:
A B C D A B C D A B C D...      Works equally well on
â†“                                all patterns
Worst-case performance!          No pathological cases
(0% hit rate)
```

**Example:** The LRU (Least Recently Used) page replacement algorithm performs terribly on sequential-cyclic workloads. Random replacement doesn't have this weaknessâ€”it performs consistently (if not optimally) across all workloads.

#### 2ï¸âƒ£ Lightweight State

```
Fair-Share with Accounting       Lottery Scheduling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Per-process state:               Per-process state:
â€¢ Total CPU consumed: 10.5s      â€¢ Number of tickets: 75
â€¢ Target share: 25%
â€¢ Current deficit: -2.3s         That's it! ğŸ‰
â€¢ Historical average: ...
â€¢ Priority adjustments: ...
```

Traditional fair-share schedulers need extensive per-process accounting. Lottery only needs ticket countsâ€”much simpler!

#### 3ï¸âƒ£ Speed

**Generating a random number is fast:**

```c
// Very fast pseudo-random generation
int random = (state * 1103515245 + 12345) & 0x7fffffff;
int winner = random % total_tickets;
```

No complex calculations, no sorting, no searching through histories. Just generate a number and pick the winner.

> **ğŸ’¡ Insight**
>
> The **randomness tradeoff** appears throughout computer science:
> - **Quicksort**: Random pivot selection avoids O(nÂ²) worst case
> - **Skip lists**: Randomized levels give expected O(log n) search
> - **Hash tables**: Random hash functions prevent adversarial inputs
> - **Load balancing**: Random server selection prevents overload
>
> When you need simplicity, robustness, and good average-case performance, **randomness is often the answer**.

**The ticket abstraction is universally powerful:**

Tickets can represent shares of **any** resource:

```
Resource Type         Ticket Application
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’» CPU time          â†’ Process gets 25% of CPU cycles
ğŸ’¾ Memory            â†’ VM gets 4GB out of 16GB total
ğŸŒ Network           â†’ Connection gets 100Mbps of 1Gbps link
ğŸ’¿ Disk I/O          â†’ App gets 30% of disk bandwidth
```

This abstraction appears in modern systems:
- **VMware ESX Server**: Tickets for memory shares between VMs
- **Lottery scheduling**: Original CPU time application
- **Network QoS**: Bandwidth allocation
- **Storage QoS**: IOPS allocation

---

## 3. ğŸ”§ Ticket Mechanisms

Beyond basic lottery drawing, tickets support sophisticated manipulation mechanisms that make the system more flexible and powerful.

### 3.1. ğŸ’± Ticket Currency

**In plain English:** Imagine you're managing an international company ğŸŒ where different departments have budgets in different currencies (USD, EUR, JPY). Each department manager allocates their budget to projects internally. The CFO converts everything to a common global currency for company-wide decisions. Ticket currency works the same way.

**How it works:**

**Users** receive tickets in **global currency**. They can allocate tickets among their jobs in their own **local currency**. The system automatically converts local â†’ global for scheduling decisions.

**Example scenario:**

```
Global Allocation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User A: 100 tickets (global)
User B: 100 tickets (global)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 200 tickets (global)


User A's Internal Allocation (A's currency)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job A1: 500 tickets (A's currency)
Job A2: 500 tickets (A's currency)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 1000 tickets (A's currency)

Conversion: A1 gets (500/1000) Ã— 100 = 50 global tickets
           A2 gets (500/1000) Ã— 100 = 50 global tickets


User B's Internal Allocation (B's currency)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job B1: 10 tickets (B's currency)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 10 tickets (B's currency)

Conversion: B1 gets (10/10) Ã— 100 = 100 global tickets


Final Global Schedule
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A1: 50 tickets  (25% of CPU)
A2: 50 tickets  (25% of CPU)
B1: 100 tickets (50% of CPU)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 200 tickets
```

**Why this matters:**
- âœ… Users don't need to coordinate with each other
- âœ… Each user allocates naturally (50/50, 70/30, etc.) in their own units
- âœ… System handles conversion automatically
- âœ… Adding a third job in User A's allocation doesn't require understanding global totals

**Visual representation:**

```
User A (100 global)              User B (100 global)
â”œâ”€â”€ A1: 500 local                â””â”€â”€ B1: 10 local
â”‚   â””â”€> 50 global                    â””â”€> 100 global
â””â”€â”€ A2: 500 local
    â””â”€> 50 global

Lottery draw over: [A1: 0-49][A2: 50-99][B1: 100-199]
```

### 3.2. ğŸ Ticket Transfer

**In plain English:** Imagine you're in line at a coffee shop â˜•. You have a meeting in 5 minutes, so you give your "priority tickets" to the barista temporarily. They serve you faster, then give your tickets back. Ticket transfer lets processes temporarily donate priority to helpers.

**Classic use case: Client-Server**

```
Initial State
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client C: 100 tickets
Server S: 50 tickets


Step 1: Client sends request to Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
C â†’ S: "Please process my database query"


Step 2: Client transfers tickets to Server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client C: 0 tickets     (temporarily gave them up)
Server S: 150 tickets   (received client's tickets)

Server now highly likely to win lottery!
Processes client's request quickly.


Step 3: Server completes work and returns tickets
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client C: 100 tickets   (tickets returned)
Server S: 50 tickets    (back to normal)
```

**Timeline visualization:**

```
Time â†’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client:  Running[100] â”€â†’ Blocked[0] â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ready[100]
           â†“             (waiting)                  â†‘
         Request      Server working             Response
           â†“                                        â†‘
Server:  Ready[50] â”€â”€â†’ Running[150] â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ready[50]
                       (boosted!)

Transferred tickets: [=========>]
                                 [<=========]
                                  Returned
```

**Why this matters:**
- âš¡ **Minimizes response time** for the client
- ğŸ¯ **Priority inheritance** happens automatically
- ğŸš« **Prevents priority inversion** (low-priority server blocking high-priority client)
- ğŸ’ª **Self-optimizing system** - processes that need to finish quickly get temporary boosts

### 3.3. ğŸ“ˆ Ticket Inflation

**In plain English:** Imagine a family ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ deciding how to split dessert ğŸ°. If everyone trusts each other, you can just say "I'm extra hungry today, give me a bigger slice" without formal negotiation. Ticket inflation lets **trusted** processes adjust their own priority on the fly.

**How it works:**

A process can **temporarily** raise or lower its ticket count:

```
Process State                    Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€

Initial: 100 tickets             Normal priority

Realizes it needs more CPU       inflate(200)
â†’ Now has: 200 tickets           Higher priority!

Finishes intensive work          inflate(50)
â†’ Now has: 50 tickets            Lower priority (nice!)
```

**Important constraints:**

```
âœ… SAFE: Cooperative environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Group of processes from same user
â€¢ Trusted processes that won't cheat
â€¢ Example: Multiple threads of same app

âŒ DANGEROUS: Competitive environment
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Untrusted users competing for CPU
â€¢ Malicious process could:
    inflate(999999999)  // Monopolize CPU!
â€¢ Breaks fairness completely
```

**Example use case:**

```
Video Player Application (3 threads)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Decoder Thread:
  â€¢ inflate(200) when decoding 4K video
  â€¢ inflate(50) when paused

UI Thread:
  â€¢ inflate(150) when user is dragging timeline
  â€¢ inflate(50) when idle

Network Thread:
  â€¢ inflate(100) when buffering
  â€¢ inflate(25) when buffer full
```

These threads **trust each other** (same application), so self-adjustment is safe and efficientâ€”no need to ask the OS for permission each time.

> **ğŸ’¡ Insight**
>
> Ticket currency, transfer, and inflation demonstrate **mechanism vs. policy** separation:
> - **Mechanism**: How tickets work (draw, convert, transfer, inflate)
> - **Policy**: When to use each mechanism (application decides)
>
> The OS provides flexible mechanisms; applications implement policies suited to their needs. This is a hallmark of good system designâ€”the same pattern that makes UNIX pipes powerful (mechanism: byte streams; policy: what to do with them).

---

## 4. ğŸ’» Implementation

### 4.1. ğŸ“Š Data Structures

**The simplest approach: A linked list of processes**

```c
typedef struct node {
    int tickets;           // Number of tickets this process owns
    struct proc *process;  // Pointer to actual process structure
    struct node *next;     // Next process in list
} node_t;
```

**Visual representation:**

```
Process List (head pointer)
â”‚
â”œâ”€â†’ [Job A: 100 tickets] â”€â†’ [Job B: 50 tickets] â”€â†’ [Job C: 250 tickets] â”€â†’ NULL
    â”‚                        â”‚                       â”‚
    â””â”€> Process A PCB        â””â”€> Process B PCB       â””â”€> Process C PCB
```

**Key information we need to track:**

```c
struct lottery_scheduler {
    node_t *head;          // First process in list
    int total_tickets;     // Sum of all tickets (400 in example)
};
```

### 4.2. ğŸ¯ Scheduling Algorithm

**The complete lottery scheduling decision code:**

```c
int counter = 0;

// winner: random number in range [0, totaltickets-1]
int winner = getrandom(0, totaltickets);

// Walk through the list, accumulating tickets
node_t *current = head;
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break;  // Found the winner!
    current = current->next;
}

// 'current' now points to the winning process
schedule(current->process);
```

**Step-by-step example:**

```
Setup:
â”€â”€â”€â”€â”€â”€
head â†’ [A:100] â†’ [B:50] â†’ [C:250] â†’ NULL
Total tickets: 400
Random winner: 300


Execution trace:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Iteration 1:
  current = A
  counter = 0 + 100 = 100
  Is 100 > 300? No
  Continue...

Iteration 2:
  current = B
  counter = 100 + 50 = 150
  Is 150 > 300? No
  Continue...

Iteration 3:
  current = C
  counter = 150 + 250 = 400
  Is 400 > 300? Yes! âœ“
  Break and schedule C
```

**Ticket ranges visualization:**

```
Ticket Number Line
0                    100    150              400
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         A            â”‚  B   â”‚       C        â”‚
â”‚     (100 tickets)    â”‚ (50) â”‚  (250 tickets) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†‘
                              Winner: 300
                              (Falls in C's range)
```

**Why this algorithm works:**

Each process effectively owns a **range** of ticket numbers:
- A owns tickets **0-99** (100 tickets)
- B owns tickets **100-149** (50 tickets)
- C owns tickets **150-399** (250 tickets)

We pick a random ticket and find which range it falls intoâ€”that's our winner!

### 4.3. âš¡ Optimization Strategies

#### ğŸ”„ Sorted List Optimization

**Problem:** If tickets are distributed unevenly, we might traverse many low-ticket processes before finding the winner.

**Solution:** Sort list by ticket count (highest first)

```
Before Sorting (worst case for winner=350):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
head â†’ [A:10] â†’ [B:15] â†’ [C:25] â†’ [D:350] â†’ NULL
       â†“         â†“         â†“         â†“
     Iterate   Iterate   Iterate   FOUND! (4 iterations)


After Sorting (best case):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
head â†’ [D:350] â†’ [C:25] â†’ [B:15] â†’ [A:10] â†’ NULL
       â†“
     FOUND! (1 iteration often)
```

**Trade-off analysis:**

```
Unsorted List                    Sorted List
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Fast insertion: O(1)          âŒ Slower insertion: O(n)
âŒ Slow scheduling: O(n) avg     âœ… Fast scheduling: O(1) often
ğŸ“ Good when: tickets equal      ğŸ“ Good when: tickets skewed
```

#### ğŸ² Random Number Generation

**Naive approach (WRONG):**

```c
// DON'T DO THIS!
int winner = rand() % total_tickets;
```

**Problem:** If `total_tickets` is not a divisor of `RAND_MAX`, you get **biased** results. Some tickets have higher probability of being selected!

**Correct approach:**

```c
// Proper range generation
int getrandom(int min, int max) {
    int range = max - min + 1;
    int limit = RAND_MAX - (RAND_MAX % range);
    int r;

    do {
        r = rand();
    } while (r >= limit);  // Reject biased values

    return min + (r % range);
}
```

> **ğŸ’¡ Insight**
>
> **The biased random number problem** is subtle and easy to miss. It's related to the **modulo bias** issue:
>
> If `RAND_MAX = 10` and `range = 3`:
> - Values 0-9 are generated
> - After `% 3`: We get 0,1,2,0,1,2,0,1,2,0
> - Result: 0 appears 4 times, 1 and 2 appear 3 times each
> - **Not uniform!**
>
> This same issue appears in cryptography, statistics, and game development. Always be careful when mapping one range to another!

---

## 5. ğŸ“Š Fairness Analysis

### 5.1. ğŸ“ Short-Term vs Long-Term Fairness

**In plain English:** Flip a fair coin ğŸª™ 10 times. You might get 7 heads and 3 tails (70/30). Not very fair! But flip it 10,000 times and you'll get very close to 5,000 heads and 5,000 tails (50/50). Lottery scheduling is the sameâ€”fairness emerges over time.

**Defining fairness:**

```
Fairness Metric (F)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
F = Time when first job completes
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Time when second job completes

Perfect fairness: F = 1.0 (both finish simultaneously)
Poor fairness: F â†’ 0 (huge gap in completion times)
```

**Example calculation:**

```
Scenario: Two jobs, equal tickets (100 each), same runtime R

Case 1: R = 10 time slices
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job 1 finishes at time: 10
Job 2 finishes at time: 20
F = 10/20 = 0.5 (not very fair!)


Case 2: R = 100 time slices
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job 1 finishes at time: 100
Job 2 finishes at time: 200
F = 100/200 = 0.5 (still not perfect, but closer)


Case 3: R = 1000 time slices
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Job 1 finishes at time: 1005
Job 2 finishes at time: 1995
F = 1005/1995 â‰ˆ 0.503 (much better!)
```

### 5.2. ğŸ“ˆ Experimental Results

**Fairness vs. job length study:**

```
Study Parameters:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Two jobs with equal tickets (100 each)
â€¢ Same runtime R (varied from 1 to 1000)
â€¢ 30 trials per configuration
â€¢ Measure: Average fairness F


Results Visualization:

Fairness (F)
1.0 â”¤                            â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
    â”‚                        â–“â–“â–“â–“
    â”‚                    â–“â–“â–“â–“
0.8 â”¤                â–“â–“â–“â–“
    â”‚            â–“â–“â–“â–“
    â”‚        â–“â–“â–“â–“
0.6 â”¤    â–“â–“â–“â–“
    â”‚ â–“â–“â–“
    â”‚â–“â–“
0.4 â”¤â–“
    â”‚
    â”‚
0.2 â”¤
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     1    10    100         1000          Job Length (R)


Key Observations:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ R = 1:    F â‰ˆ 0.5  (High variance, poor fairness)
â€¢ R = 10:   F â‰ˆ 0.65 (Improving...)
â€¢ R = 100:  F â‰ˆ 0.85 (Much better)
â€¢ R = 1000: F â‰ˆ 0.95 (Nearly perfect!)
```

**Why this happens:**

```
Short Jobs (R = 10)              Long Jobs (R = 1000)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Sample execution:                Sample execution:
A A A A A A A B B B             A B A B A B A B A B A B...
â†‘                                â†‘
High variance                    Averages out
(random clusters)                (law of large numbers)

Few lottery draws = lucky        Many lottery draws = fair
streaks possible                 distribution emerges
```

> **ğŸ’¡ Insight**
>
> This illustrates the **Law of Large Numbers** from probability theory: As sample size increases, the sample average converges to the expected value. In our case:
> - **Expected value**: 50% CPU for each job
> - **Short runs**: High variance, might get 70/30
> - **Long runs**: Low variance, approaches 50/50
>
> This same principle underlies Monte Carlo simulations, A/B testing, and pollingâ€”you need sufficient samples for statistical validity!

---

## 6. ğŸ¯ Stride Scheduling

### 6.1. âœ… Deterministic Fair-Share

**In plain English:** Lottery scheduling uses randomness, so there's always some uncertainty. What if you want **perfect** fairnessâ€”guaranteed? That's stride scheduling. Instead of drawing random tickets, we carefully track exactly how much CPU each process has used and always pick the one that's gotten the least so far.

**Core concept:**

Each process has two values:
1. **Stride**: Inverse of tickets (how much to increment each time)
2. **Pass**: Current progress counter (starts at 0)

**Formula:**

```
Stride Calculation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
stride = LARGE_NUMBER / tickets

Example with LARGE_NUMBER = 10,000:
Job A (100 tickets): stride = 10,000 / 100 = 100
Job B (50 tickets):  stride = 10,000 / 50  = 200
Job C (250 tickets): stride = 10,000 / 250 = 40
```

**Algorithm:**

```
At each scheduling decision:
1. Pick process with LOWEST pass value
2. Run that process for one time slice
3. Increment its pass by its stride
4. Repeat
```

### 6.2. ğŸ”„ Complete Stride Scheduling Example

**Initial setup:**

```
Process   Tickets   Stride   Pass
â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€
A         100       100      0
B         50        200      0
C         250       40       0
```

**Execution trace:**

```
Step   A_Pass   B_Pass   C_Pass   Who Runs?   Why?
â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€   -â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0      0        0        0        A*          Tie, pick any
1      100      0        0        B           0 < 100
2      100      200      0        C           0 < 100 < 200
3      100      200      40       C           40 < 100 < 200
4      100      200      80       C           80 < 100 < 200
5      100      200      120      A           100 < 120 < 200
6      200      200      120      C           120 < 200 = 200
7      200      200      160      C           160 < 200 = 200
8      200      200      200      A,B,C       All tied!
9      300      200      200      B           200 < 200 < 300
...

Execution sequence: A B C C C A C C (A B C) ...
                                    â””â”€â”€â”¬â”€â”€â”˜
                                  Cycle repeats
```

**After one complete cycle (pass values all equal):**

```
Jobs Run Count (in one cycle)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
C ran: 5 times  (250/400 = 62.5%) âœ“
A ran: 2 times  (100/400 = 25.0%) âœ“
B ran: 1 time   (50/400  = 12.5%) âœ“

PERFECT proportional sharing! ğŸ¯
```

**Visual timeline:**

```
Time Slice   1   2   3   4   5   6   7   8   9   10  11  12  ...
             â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚
Execution    A   B   C   C   C   A   C   C   A   B   C   C  ...
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     Cycle 1                    Cycle 2

Proportions after cycle 1:
C: 5/8 = 62.5% (target: 62.5%) âœ“
A: 2/8 = 25.0% (target: 25.0%) âœ“
B: 1/8 = 12.5% (target: 12.5%) âœ“
```

> **ğŸ’¡ Insight**
>
> Stride scheduling is **deterministic** while lottery is **probabilistic**:
>
> | Aspect          | Lottery              | Stride               |
> |-----------------|----------------------|----------------------|
> | Fairness        | Eventually converges | Perfect every cycle  |
> | Complexity      | Very simple          | Slightly complex     |
> | State required  | Just ticket counts   | Pass values needed   |
> | Predictability  | Cannot predict       | Fully deterministic  |
> | Adding process  | Easy (no global state)| Hard (what pass value?) |
>
> This trade-off between **simplicity and precision** appears everywhere in CS: lossy vs. lossless compression, approximate vs. exact algorithms, eventual vs. strong consistency.

### 6.3. ğŸ†š Stride vs Lottery Tradeoffs

**The global state problem:**

```
Scenario: New process D arrives mid-execution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Current state:
A: pass = 500
B: pass = 800
C: pass = 350

â“ What should D's initial pass value be?

Option 1: Start at 0
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
D: pass = 0  (stride = 100)

Problem: D will monopolize CPU!
A(500) B(800) C(350) D(0) â†’ Always picks D
D runs forever until pass catches up to others
âš ï¸ Unfair to existing processes!


Option 2: Start at current minimum
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
D: pass = 350  (same as C)

Better, but still complex:
â€¢ Need to track global minimum
â€¢ What if C exits? Recalculate minimum
â€¢ Requires coordination across all processes
```

**Lottery scheduling advantage:**

```
Same scenario with lottery:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Current state:
A: 100 tickets
B: 50 tickets
C: 250 tickets
Total: 400 tickets

New process D arrives:
D: 75 tickets

Simply update:
Total: 400 + 75 = 475 tickets

Next lottery draw is over 475 tickets.
D immediately has fair chance! ğŸ‰

No coordination needed!
No global state to manage!
```

**When to use each:**

```
Use Lottery Scheduling When:            Use Stride Scheduling When:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Processes arrive/leave frequently    âœ… Static set of processes
âœ… Approximate fairness is acceptable   âœ… Perfect fairness required
âœ… Simplicity is paramount              âœ… Predictability matters
âœ… Low overhead needed                  âœ… Can tolerate more complexity

Examples:                               Examples:
â€¢ Interactive systems                   â€¢ Real-time systems
â€¢ Cloud multi-tenancy                   â€¢ Video encoders
â€¢ General-purpose OS                    â€¢ Scientific computing
```

> **ğŸ’¡ Insight**
>
> The lottery vs. stride choice exemplifies **engineering trade-offs**:
> - Lottery: Optimized for **dynamism** (easy to add/remove processes)
> - Stride: Optimized for **precision** (exact fairness guarantees)
>
> Neither is "better"â€”they serve different needs. This is like choosing between:
> - HashMap (fast average case) vs. TreeMap (guaranteed worst case)
> - UDP (low overhead) vs. TCP (reliability)
> - NoSQL (availability) vs. SQL (consistency)
>
> Understanding when to favor simplicity vs. precision is a key engineering skill.

---

## 7. ğŸ§ Linux Completely Fair Scheduler (CFS)

**In plain English:** CFS is Linux's production schedulerâ€”what runs on billions of devices ğŸ“±ğŸ’»ğŸ–¥ï¸ worldwide. It's like stride scheduling's practical cousin: deterministic fairness, but engineered for **massive scale** and **efficiency**. Instead of tickets and passes, it uses "virtual runtime" and clever data structures to minimize scheduling overhead.

### 7.1. â±ï¸ Virtual Runtime

**Core idea: Track how much CPU each process has consumed**

```
Virtual Runtime (vruntime)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Every process accumulates vruntime as it runs.
Scheduler always picks process with LOWEST vruntime.

Simple, elegant! ğŸ¯
```

**How it works:**

```
Initial State (all processes start)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Process A: vruntime = 0
Process B: vruntime = 0
Process C: vruntime = 0


After A runs for 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Process A: vruntime = 10ms  â† Ran for 10ms
Process B: vruntime = 0ms   â† Hasn't run yet (LOWEST!)
Process C: vruntime = 0ms   â† Hasn't run yet

Scheduler picks B (lowest vruntime)


After B runs for 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Process A: vruntime = 10ms
Process B: vruntime = 10ms  â† Now caught up
Process C: vruntime = 0ms   â† Still hasn't run (LOWEST!)

Scheduler picks C


After C runs for 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Process A: vruntime = 10ms  â† LOWEST (tied)
Process B: vruntime = 10ms  â† LOWEST (tied)
Process C: vruntime = 10ms  â† LOWEST (tied)

All equal! Pick any (round-robin among tied)
```

**Visualization over time:**

```
A B C D process labels
â”‚ â”‚ â”‚ â”‚
â”œâ”€â”¬â”€â”¬â”€â”¤
â”‚ â”‚ â”‚ â”‚
â””â”€â”´â”€â”´â”€â”˜
  0ms    Initial state (all vruntime = 0)

    A runs 12ms
    â†“
â”Œâ”€â”€â”€â”¬â”€â”¬â”€â”
â”‚   â”‚ â”‚ â”‚
â”œâ”€â”€â”€â”¼â”€â”¼â”€â”¤
â”‚12 â”‚0â”‚0â”‚
â””â”€â”€â”€â”´â”€â”´â”€â”˜
        B runs 12ms (lowest)
        â†“
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”
â”‚   â”‚   â”‚ â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”¤
â”‚12 â”‚12 â”‚0â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”˜
            C runs 12ms (lowest)
            â†“
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚12 â”‚12 â”‚12 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
  All equal â†’ round-robin
```

### 7.2. âš™ï¸ Time Slice Calculation

**The dynamic time slice formula:**

Unlike traditional schedulers with fixed time slices (e.g., 10ms), CFS computes time slices **dynamically** based on the number of running processes.

```
Control Parameters
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sched_latency:   Target period for fairness (default: 48ms)
min_granularity: Minimum time slice (default: 6ms)


Time Slice Calculation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_slice = sched_latency / n

where n = number of running processes
```

**Example scenarios:**

```
Scenario 1: 4 processes running
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_slice = 48ms / 4 = 12ms per process

Timeline (48ms period):
0ms      12ms     24ms     36ms     48ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   A    â”‚   B    â”‚   C    â”‚   D    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
All processes run once in 48ms âœ“ (perfectly fair!)


Scenario 2: 2 processes finish, now 2 remain
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_slice = 48ms / 2 = 24ms per process

Timeline (48ms period):
0ms           24ms              48ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      A      â”‚        B        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Processes get longer slices (fewer context switches!)


Scenario 3: 10 processes running
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_slice = 48ms / 10 = 4.8ms

âš ï¸ BUT: 4.8ms < min_granularity (6ms)
â†’ CFS overrides: time_slice = 6ms per process

Timeline (60ms actual period, not 48ms):
0ms   6   12  18  24  30  36  42  48  54  60ms
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ A â”‚ B â”‚ C â”‚ D â”‚ E â”‚ F â”‚ G â”‚ H â”‚ I â”‚ J â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Total period: 10 Ã— 6ms = 60ms (not 48ms)
Trade-off: Slightly less fair, but acceptable overhead
```

**Why min_granularity matters:**

```
Without minimum (pathological case):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1000 processes running
time_slice = 48ms / 1000 = 0.048ms (48 microseconds!)

Problems:
âš ï¸ Context switch cost: ~1-5 microseconds
âš ï¸ Time slice: 48 microseconds
â†’ Spending 10%+ of time just switching! (terrible overhead)


With minimum (actual CFS):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_slice = max(48ms / 1000, 6ms) = 6ms

Total period: 1000 Ã— 6ms = 6 seconds
Trade-off: Not perfectly fair over 48ms, but:
âœ… Still reasonably fair over 6 seconds
âœ… Acceptable context switch overhead
```

> **ğŸ’¡ Insight**
>
> CFS's **adaptive time slice** demonstrates a fundamental OS principle: **balance between fairness and efficiency**.
>
> - **Smaller time slices** â†’ More fair (everyone gets frequent turns)
> - **Larger time slices** â†’ More efficient (less context switching)
>
> This is the same trade-off in:
> - **Network packet size**: Small packets (low latency) vs. large packets (high throughput)
> - **Database transactions**: Short transactions (high concurrency) vs. long transactions (less overhead)
> - **Garbage collection**: Frequent small GC pauses vs. infrequent large pauses

### 7.3. âš–ï¸ Process Weighting (Nice Values)

**In plain English:** Not all processes are equal. Your video call ğŸ“¹ should get more CPU than a background file sync. The UNIX `nice` command lets you adjust priority: "be nice to others" (positive nice = lower priority) or "I'm important" (negative nice = higher priority).

**Nice value range:**

```
Priority Level        Nice Value    Priority
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€
Highest priority      -20           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                      -15           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                      -10           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                      -5            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Default               0             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                      +5            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                      +10           â–ˆâ–ˆâ–ˆâ–ˆ
                      +15           â–ˆâ–ˆ
Lowest priority       +19           â–ˆ
```

**Weight mapping (from Linux kernel):**

```c
static const int prio_to_weight[40] = {
  /* -20 */  88761, 71755, 56483, 46273, 36291,
  /* -15 */  29154, 23254, 18705, 14949, 11916,
  /* -10 */   9548,  7620,  6100,  4904,  3906,
  /*  -5 */   3121,  2501,  1991,  1586,  1277,
  /*   0 */   1024,   820,   655,   526,   423,
  /*  +5 */    335,   272,   215,   172,   137,
  /* +10 */    110,    87,    70,    56,    45,
  /* +15 */     36,    29,    23,    18,    15,
};
```

**Key property:** Each 1 nice value change â‰ˆ **10% CPU change**

```
Nice Differences and CPU Ratios
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Nice 0  vs Nice +1:   1024 vs 820  â‰ˆ 1.25x difference
Nice 0  vs Nice +5:   1024 vs 335  â‰ˆ 3x difference
Nice 0  vs Nice +10:  1024 vs 110  â‰ˆ 9.3x difference
Nice -5 vs Nice 0:    3121 vs 1024 â‰ˆ 3x difference
Nice -10 vs Nice +10: 9548 vs 110  â‰ˆ 86x difference!
```

**Weighted time slice calculation:**

```
Formula:
â”€â”€â”€â”€â”€â”€â”€
time_slice_k = (weight_k / Î£(all weights)) Ã— sched_latency


Example: Two processes with different priorities
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Process A: nice = -5  â†’  weight = 3121
Process B: nice = 0   â†’  weight = 1024

Total weight = 3121 + 1024 = 4145

A's time slice = (3121 / 4145) Ã— 48ms â‰ˆ 36ms (75%)
B's time slice = (1024 / 4145) Ã— 48ms â‰ˆ 12ms (25%)


Timeline:
0ms              36ms     48ms
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       A        â”‚   B    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

A gets 3Ã— more CPU than B! ğŸ¯
```

**Weighted vruntime accumulation:**

Without weighting, vruntime increases at the same rate for everyone:
```
vruntime += runtime
```

With weighting, lower-priority processes accumulate vruntime **faster**:

```c
// Actual CFS formula
vruntime_i = vruntime_i + (weight_0 / weight_i) Ã— runtime_i

where weight_0 = 1024 (nice 0 weight)
```

**Example:**

```
Process A (nice -5, weight 3121) runs 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vruntime_A += (1024 / 3121) Ã— 10ms
vruntime_A += 0.328 Ã— 10ms
vruntime_A += 3.28ms  (accumulates slowly!)


Process B (nice 0, weight 1024) runs 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vruntime_B += (1024 / 1024) Ã— 10ms
vruntime_B += 1.0 Ã— 10ms
vruntime_B += 10ms  (normal accumulation)


Process C (nice +5, weight 335) runs 10ms:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vruntime_C += (1024 / 335) Ã— 10ms
vruntime_C += 3.06 Ã— 10ms
vruntime_C += 30.6ms  (accumulates quickly!)
```

**Why this works:**

```
Scenario: A (weight 3121) and B (weight 1024) compete
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Initial state:
A: vruntime = 0
B: vruntime = 0

Round 1: Pick A (lowest vruntime, tied)
A runs 10ms â†’ vruntime_A = 3.28ms

Round 2: Pick B (vruntime 0 < 3.28)
B runs 10ms â†’ vruntime_B = 10ms

Round 3: Pick A (vruntime 3.28 < 10)
A runs 10ms â†’ vruntime_A = 6.56ms

Round 4: Pick A (vruntime 6.56 < 10)
A runs 10ms â†’ vruntime_A = 9.84ms

Round 5: Pick A (vruntime 9.84 < 10)
A runs 10ms â†’ vruntime_A = 13.12ms

Round 6: Pick B (vruntime 10 < 13.12)
B runs 10ms â†’ vruntime_B = 20ms

Pattern: A runs 3Ã— as often as B!
A A B A A A B A A A B ...
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
   3:1       3:1
```

> **ğŸ’¡ Insight**
>
> CFS's **weighting system** is beautifully elegant:
> 1. Higher priority â†’ Lower weight â†’ vruntime accumulates slower â†’ Stays low longer â†’ Gets picked more often
> 2. The math naturally produces the desired CPU ratios without explicit percentages
> 3. Adding/removing processes automatically adjusts everyone's share
>
> This is an example of **designing the right metric**â€”vruntime naturally encodes fairness when weighted correctly. Compare to manual tracking: "A has used 30%, B has used 20%, C is owed 10% more..."â€”much more complex!

### 7.4. ğŸŒ³ Red-Black Trees

**The scalability problem:**

```
Naive approach: Store processes in a list
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

head â†’ [vruntime: 100] â†’ [vruntime: 200] â†’ ... â†’ [vruntime: 9999]

Finding minimum: O(1) if sorted, but...
Inserting process: O(n) to maintain sort
Removing process: O(n) to search

With 1000s of processes, this is too slow! ğŸŒ
```

**CFS solution: Red-Black Tree**

A red-black tree is a **self-balancing binary search tree** with O(log n) operations.

```
Properties:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Each node is red or black
â€¢ Root is black
â€¢ All leaves (NIL) are black
â€¢ Red nodes have black children (no consecutive reds)
â€¢ All paths from node to leaves have same number of black nodes


Example CFS red-black tree (vruntime values):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

               [14] B
              /     \
         [9] R       [18] R
        /    \       /     \
    [1] B  [10] B [17] B  [22] B
     /  \               /
  [5] R [NIL]       [21] R
                    /
                [24] R


Legend:
B = Black node
R = Red node
Numbers = vruntime values
```

**Key operations:**

```
Operation         List      Red-Black Tree
â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Find minimum      O(n)      O(log n)  â† Just leftmost node!
Insert            O(n)      O(log n)
Delete            O(n)      O(log n)
Search            O(n)      O(log n)
```

**CFS scheduling with red-black tree:**

```c
// Simplified CFS scheduler

struct rb_node *pick_next_task() {
    // Leftmost node = minimum vruntime
    struct rb_node *leftmost = rb_first(&cfs_rq);
    return leftmost;  // O(log n) to find!
}

void enqueue_task(struct task *p) {
    // Insert task into red-black tree by vruntime
    rb_insert(&cfs_rq, p, p->vruntime);  // O(log n)
}

void dequeue_task(struct task *p) {
    // Remove task from red-black tree
    rb_erase(&cfs_rq, p);  // O(log n)
}
```

**Performance comparison:**

```
System Load: 1000 active processes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

List-based scheduler:
  Find next task:     1000 comparisons
  Insert finished:    500 comparisons (avg)
  Per scheduling:     ~1500 operations

Red-black tree scheduler (CFS):
  Find next task:     1 operation (cached leftmost!)
  Insert finished:    ~10 operations (logâ‚‚ 1000 â‰ˆ 10)
  Per scheduling:     ~11 operations

Speedup: 1500 / 11 â‰ˆ 136Ã— faster! ğŸš€
```

**Optimizations in production CFS:**

```
CFS actually caches the leftmost node!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

struct cfs_rq {
    struct rb_root tasks_timeline;
    struct rb_node *rb_leftmost;  â† Cache!
    ...
};

Why?
â”€â”€â”€â”€â”€
â€¢ Finding minimum is the MOST frequent operation
â€¢ Instead of O(log n), cached leftmost is O(1)
â€¢ Update cache when leftmost changes (rare)

Result: Finding next task is effectively FREE! âš¡
```

> **ğŸ’¡ Insight**
>
> **Data structure choice is critical for performance.** CFS demonstrates this beautifully:
>
> Early schedulers: Simple lists, worked fine for 10-100 processes
> Modern data centers: 1000-10000 processes, lists are too slow
> Solution: Red-black tree with caching
>
> This pattern appears everywhere:
> - **Web servers**: Array of connections â†’ hash table of connections
> - **Databases**: Sequential scan â†’ B-tree index â†’ bitmap index
> - **Network routing**: Linear search â†’ trie â†’ compressed trie
>
> As scale increases, algorithm complexity matters more than constant factors. O(n) vs. O(log n) is the difference between "barely works" and "handles millions."

### 7.5. ğŸ˜´ Handling I/O and Sleeping Processes

**The starvation problem:**

```
Scenario: Process goes to sleep for 10 seconds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Process A (CPU-bound):    vruntime continuously increasing
  Time 0s:  vruntime = 0
  Time 5s:  vruntime = 5000ms
  Time 10s: vruntime = 10000ms
  Always running!

Process B (I/O-bound):    Goes to sleep, vruntime frozen
  Time 0s:  vruntime = 0
  Time 0s:  [Starts I/O, sleeps]
  Time 10s: [I/O completes, wakes up]
            vruntime = 0  (unchanged!)


âŒ Problem: When B wakes up...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
B has vruntime = 0
A has vruntime = 10000ms

CFS always picks lowest vruntime â†’ B runs
B "catches up" for 10 seconds straight!
A is starved! ğŸ˜±
```

**Visual timeline of the problem:**

```
Time (seconds)
0              5              10             20
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
A: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
B:                            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                              â†‘
                         B wakes up,
                      monopolizes CPU
                      for 10 seconds
```

**CFS solution: Reset vruntime on wakeup**

```c
void enqueue_task_fair(struct task *p) {
    if (p->state == TASK_WAKING) {
        // Process is waking up from sleep
        u64 min_vruntime = cfs_rq->min_vruntime;

        // Set waking process to minimum of tree
        p->vruntime = max(p->vruntime, min_vruntime);
    }

    rb_insert(&cfs_rq, p);
}
```

**Corrected behavior:**

```
Process B wakes up at time 10s:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
min_vruntime in tree = 10000ms (from A)

B's vruntime adjusted:
  Old vruntime: 0ms
  New vruntime: max(0, 10000) = 10000ms

Now both have equal vruntime:
  A: vruntime = 10000ms
  B: vruntime = 10000ms

They compete fairly! âœ“
```

**Timeline with fix:**

```
Time (seconds)
0              5              10             15
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
A: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
B:                            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                              â†‘
                         B wakes up,
                      competes fairly
                      with A (50/50)
```

**Trade-offs of this approach:**

```
âœ… Advantages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Prevents starvation of CPU-bound processes
â€¢ Maintains overall system fairness
â€¢ Simple to implement

âŒ Disadvantages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ I/O-bound processes don't accumulate "credit"
â€¢ Short sleeps are penalized
â€¢ Can reduce responsiveness for interactive apps


Example problem:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Interactive text editor:
  Runs for 1ms (process input)
  Sleeps for 99ms (waiting for next keystroke)
  Repeat...

With vruntime reset:
  Never accumulates "credit" for sleeping
  Competes equally with CPU hogs
  May feel sluggish! âš ï¸
```

**Academic paper findings:**

According to Ousterhout's research [AC97], CFS's approach of resetting vruntime means:

```
Interactive processes that sleep frequently:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Do NOT get compensated for yielding CPU
â€¢ Get same share as processes that never sleep
â€¢ May experience poor response time

Traditional schedulers with I/O boost:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Give higher priority to I/O-bound processes
â€¢ Better for interactive workloads
â€¢ More complex to implement correctly
```

> **ğŸ’¡ Insight**
>
> The **sleeping process problem** reveals a fundamental tension in scheduler design:
>
> 1. **Fairness**: All processes should get equal CPU share
> 2. **Responsiveness**: Interactive processes should respond quickly
>
> These goals conflict! Solutions:
> - **CFS approach**: Prioritize fairness, sacrifice some responsiveness
> - **Traditional approach**: Boost I/O-bound processes, sacrifice some fairness
> - **Android**: Separate "foreground" and "background" groups with different shares
> - **Windows**: Interactive boost heuristics
>
> There's no perfect answerâ€”just different trade-offs for different use cases. This is why modern OSes often have **multiple schedulers** (real-time, deadline, CFS) that applications can choose from.

---

## 8. âš ï¸ Challenges and Limitations

### ğŸŸï¸ The Ticket Assignment Problem

**The fundamental open question:**

```
Given a set of processes, how many tickets should each receive?
```

**In plain English:** Lottery and stride scheduling are elegant mechanisms, but they dodge the hardest question: "Who deserves what share?" Should your browser get 40 tickets or 400? Should background tasks get 10 or 100?

**Approaches attempted:**

```
1. User-specified allocation
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Pro: Users know their priorities
   Con: Users don't understand ticket math
        How do I know if 100 tickets is "enough"?

2. System administrator allocation
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Pro: Expert can make informed decisions
   Con: Doesn't scale to millions of processes
        Admin can't micromanage every app

3. Default equal allocation
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Pro: Simple, democratic
   Con: All processes treated equally
        Defeats the purpose of proportional share!

4. Dynamic adjustment (machine learning?)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Pro: Could learn from behavior
   Con: Complex, unpredictable, still research area
```

**Example of the difficulty:**

```
User's laptop processes:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Video call (Zoom)           â†’ ??? tickets
â€¢ Browser (20 tabs)           â†’ ??? tickets
â€¢ Text editor                 â†’ ??? tickets
â€¢ Background system updates   â†’ ??? tickets
â€¢ Music player                â†’ ??? tickets
â€¢ File sync (Dropbox)         â†’ ??? tickets

What's the "right" allocation? ğŸ¤·
â€¢ Video call needs CPU for real-time encoding
â€¢ Browser needs responsiveness when user clicks
â€¢ Background tasks shouldn't interfere
â€¢ But how to quantify this in tickets?
```

> **ğŸ’¡ Insight**
>
> The **ticket assignment problem** is an instance of a broader challenge: **translating human priorities into system parameters**. Similar problems:
> - **Database tuning**: How big should the buffer pool be?
> - **Network QoS**: What bandwidth guarantee for each service class?
> - **Memory limits**: How much RAM should each container get?
>
> Often, the answer is **adaptive systems** that adjust based on observed behaviorâ€”but this adds complexity. The tension between simplicity and optimality is eternal!

### ğŸ“‰ I/O Integration Issues

**The mismatch:**

```
Proportional share schedulers assume:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Processes constantly want CPU
â€¢ Fair share = equal time percentages


Reality:
â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Many processes are I/O-bound
â€¢ They voluntarily yield CPU (waiting for disk/network)
â€¢ Should they get "credit" for being nice?


Example:
â”€â”€â”€â”€â”€â”€â”€â”€
CPU-bound process (A):    Never yields, uses 100% of its time slices
I/O-bound process (B):    Uses 10% of its time slices (rest is I/O wait)

Both have 100 tickets (should get 50% each)

Actual CPU usage:
  A: 90% (it never sleeps)
  B: 10% (it sleeps a lot)

Is this fair? ğŸ¤”
```

**Two perspectives:**

```
Perspective 1: This IS fair
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ B got its fair share of OPPORTUNITIES to run
â€¢ B chose to do I/O instead
â€¢ The unused time goes to A (productive use)


Perspective 2: This is NOT fair
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ B should get compensated when its I/O completes
â€¢ Interactive processes are more valuable than batch
â€¢ User typing expects instant response, not fair share
```

**Why this is hard:**

```
Attempted solution: Give I/O-bound processes priority boost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problems:
âš ï¸ How much boost? (opens same parameterization problem)
âš ï¸ Can be gamed: Process does fake I/O to get priority
âš ï¸ Breaks the fairness guarantee
âš ï¸ Adds complexity

Result: Most proportional-share schedulers (including CFS)
        don't solve this well. Trade-off accepted.
```

### ğŸš« Not a Panacea

**When NOT to use proportional-share scheduling:**

```
âŒ Real-time systems
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Need: Hard deadlines ("finish in 50ms")
   Proportional share: No timing guarantees
   Use instead: EDF (Earliest Deadline First)

âŒ Batch processing
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Need: Minimize turnaround time
   Proportional share: Slows down all jobs equally
   Use instead: SJF (Shortest Job First)

âŒ Energy-constrained devices
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Need: Maximize sleep time, minimize wakeups
   Proportional share: Frequent scheduling decisions
   Use instead: Batched scheduling

âŒ Untrusted environments (no ticket inflation)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Need: Prevent gaming/cheating
   Proportional share: Trusts processes not to inflate
   Use instead: Enforced quotas
```

**Where proportional-share DOES excel:**

```
âœ… Virtual machine managers (VMware, KVM)
âœ… Cloud resource allocation (AWS, GCP)
âœ… Multi-tenant servers
âœ… Quality-of-service guarantees
âœ… Fair bandwidth/memory sharing
```

> **ğŸ’¡ Insight**
>
> **No scheduler is universal.** Each optimizes for different goals:
>
> | Scheduler Type        | Optimizes For         | Use Case            |
> |-----------------------|------------------------|---------------------|
> | FIFO                  | Simplicity             | Batch queues        |
> | SJF                   | Turnaround time        | Known-length jobs   |
> | Round-robin           | Response time          | Time-sharing        |
> | MLFQ                  | Balance                | General-purpose     |
> | Proportional-share    | Fairness               | Resource allocation |
> | EDF                   | Deadlines              | Real-time systems   |
>
> Modern OSes often support **multiple schedulers** simultaneously:
> - Linux: CFS (general), RT (real-time), Deadline
> - Windows: Priority-based with multiple queues
> - Real-time OSes: Deadline schedulers
>
> Choosing the right scheduler is part of system design, not a one-size-fits-all decision.

---

## 9. ğŸ“ Summary

**Key Takeaways:** ğŸ¯

**1. Proportional-Share Philosophy** ğŸ«
```
Shift in goals:
  Old schedulers: Minimize turnaround/response time
  Proportional:   Guarantee fair CPU percentage

Core abstraction: Tickets represent share ownership
```

**2. Three Approaches** ğŸ²

```
Lottery Scheduling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ² Random ticket drawing
âœ… Simple, lightweight
âœ… No global state
âœ… Robust against corner cases
âŒ Probabilistic fairness (not guaranteed)

Stride Scheduling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Deterministic pass/stride tracking
âœ… Perfect fairness every cycle
âœ… Predictable behavior
âŒ Global state complicates dynamic addition
âŒ More complex implementation

CFS (Linux Production)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â±ï¸ Virtual runtime with red-black trees
âœ… O(log n) efficiency at scale
âœ… Dynamic time slices
âœ… Nice value integration
âœ… Production-proven (billions of devices)
âŒ Doesn't compensate I/O-bound processes well
```

**3. Implementation Insights** ğŸ’»

```
Key techniques:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Ticket currency: User-level allocation abstraction
â€¢ Ticket transfer: Priority inheritance for client-server
â€¢ Ticket inflation: Self-adjustment in cooperative environments
â€¢ Red-black trees: O(log n) operations for scalability
â€¢ Weighted vruntime: Elegant priority integration
```

**4. Fundamental Trade-offs** âš–ï¸

```
Simplicity â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Precision
   Lottery              Stride

Fairness â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Responsiveness
   CFS vruntime reset    Traditional I/O boost

Static â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Dynamic
   Stride (hard to add)  Lottery (easy to add)
```

**5. Challenges Remaining** âš ï¸

```
â“ Ticket assignment: Who gets how many?
â“ I/O integration: Should sleeping processes get credit?
â“ Gaming prevention: How to prevent ticket manipulation?

These are POLICY questions, not mechanism questions.
The mechanisms work; the policies are domain-specific.
```

**What's Next:** ğŸš€

Upcoming chapters will explore:
- ğŸ”„ **Multiprocessor scheduling** - Fair sharing across many CPUs
- â° **Real-time scheduling** - Deadline guarantees instead of fairness
- ğŸ’¾ **Memory scheduling** - Applying proportional share to RAM
- ğŸŒ **Network scheduling** - Fair bandwidth allocation

**Conceptual Connections:** ğŸ§ 

```
Proportional-share scheduling connects to:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Economics:         Resource allocation under scarcity
Game Theory:       Fair division problems
Probability:       Law of large numbers (lottery convergence)
Data Structures:   Self-balancing trees (red-black)
Algorithms:        Randomized algorithms vs. deterministic
Distributed Sys:   Quota management, rate limiting
Networking:        Quality-of-Service (QoS), traffic shaping
```

> **ğŸ’¡ Final Insight**
>
> Proportional-share scheduling teaches us that **fairness is measurable and enforceable**, but defining "fair" is subjective. Different contexts demand different fairness models:
>
> - **Egalitarian**: Equal CPU for all (naive lottery)
> - **Weighted**: CPU proportional to priority (CFS nice values)
> - **Utility-based**: CPU proportional to value delivered (economic schedulers)
> - **Deadline-driven**: CPU to whoever is most urgent (real-time)
>
> The **mechanism** (tickets, vruntime, etc.) is the easy part. The **policy** (who deserves what) remains a sociotechnical challenge that blends computer science, economics, and human values. This is why OS design is as much art as science.

---

**Previous:** [Chapter 5: MLFQ Scheduling](chapter5-mlfq.md) | **Next:** [Chapter 7: Multiprocessor Scheduling](chapter7-multiprocessor.md)
