19 Paging: Faster Translations (TLBs) Using paging as the core mechanism to support virtual memory can lead to highperformanceoverheads. Bychoppingtheaddressspaceintosmall, f ixed-sized units (i.e., pages), paging requires a large amount of mapping information. Because that mapping information is generally stored in physical memory, paging logically requires an extra memory lookup for each virtual address generated by the program. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow. And thus our problem: THE CRUX: HOWTOSPEED UP ADDRESS TRANSLATION How can we speed up address translation, and generally avoid the extra memory reference that paging seems to require? What hardware support is required? What OS involvement is needed? When we want to make things fast, the OS usually needs some help. And help often comes from the OS’s old friend: the hardware. To speed address translation, we are going to add what is called (for historical reasons [CP78]) a translation-lookaside buffer, or TLB [CG68, C95]. A TLB is part of the chip’s memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache. Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible [C95]. 12 PAGING: FASTER TRANSLATIONS (TLBS) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 VPN = (VirtualAddress & VPN_MASK) >> SHIFT (Success, TlbEntry) = TLB_Lookup(VPN) if (Success == True) // TLB Hit if (CanAccess(TlbEntry.ProtectBits) == True) Offset = VirtualAddress & OFFSET_MASK PhysAddr = (TlbEntry.PFN << SHIFT) | Offset Register = AccessMemory(PhysAddr) else RaiseException(PROTECTION_FAULT) else // TLB Miss PTEAddr = PTBR + (VPN * sizeof(PTE)) PTE = AccessMemory(PTEAddr) if (PTE.Valid == False) RaiseException(SEGMENTATION_FAULT) else if (CanAccess(PTE.ProtectBits) == False) RaiseException(PROTECTION_FAULT) else TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) RetryInstruction() Figure 19.1: TLB Control Flow Algorithm 19.1 TLB Basic Algorithm Figure 19.1 shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple linear page table (i.e., the page table is an array) and a hardware-managed TLB (i.e., the hardware handles much of the responsibility of page table accesses; we’ll explain more about this below). The algorithm the hardware follows works like this: first, extract the virtual pagenumber(VPN)fromthevirtualaddress(Line1inFigure19.1), andcheck if the TLBholds the translation for this VPN (Line 2). If it does, we have a TLB hit, which means the TLB holds the translation. Success! Wecannowextract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (Lines 5–7), assuming protection checks do not fail (Line 4). If the CPU does not find the translation in the TLB (a TLB miss), we have some more work to do. In this example, the hardware accesses the page table to find the translation (Lines 11–12), and, assuming that the virtual memory reference generated by the process is valid and accessible (Lines 13, 15), updates the TLB with the translation (Line 18). These set of actions are costly, primarily because of the extra memory reference needed to access the page table (Line 12). Finally, once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]PAGING: FASTER TRANSLATIONS (TLBS) 3 The TLB, like all caches, is built on the premise that in the common case, translations are found in the cache (i.e., are hits). If so, little overhead is added, as the TLB is found near the processing core and is designed to be quite fast. When a miss occurs, the high cost of paging is incurred; the page table must be accessed to find the translation, and an extra memoryreference(ormore, withmorecomplexpagetables)results. If this happens often, the program will likely run noticeably more slowly; memory accesses, relative to most CPU instructions, are quite costly, and TLB misses lead to more memory accesses. Thus, it is our hope to avoid TLB misses as much as we can. 19.2 Example: Accessing An Array To make clear the operation of a TLB, let’s examine a simple virtual address trace and see how a TLB can improve its performance. In this example, let’s assume we have an array of 10 4-byte integers in memory, starting at virtual address 100. Assume further that we have a small 8-bit virtual address space, with 16-byte pages; thus, a virtual address breaks downintoa4-bit VPN(there are 16 virtual pages) and a 4-bit offset (there are 16 bytes on each of those pages). Figure 19.2 (page 4) shows the array laid out on the 16 16-byte pages of the system. As you can see, the array’s first entry (a[0]) begins on (VPN=06, offset=04); only three 4-byte integers fit onto that page. The array continues onto the next page (VPN=07), where the next four entries (a[3] ... a[6]) are found. Finally, the last three entries of the 10-entry array (a[7] ... a[9]) are located on the next page of the address space (VPN=08). Now let’s consider a simple loop that accesses each array element, something that would look like this in C: int i, sum = 0; for (i = 0; i < 10; i++) { sum += a[i]; } For the sake of simplicity, we will pretend that the only memory accesses the loop generates are to the array (ignoring the variables i and sum, as well as the instructions themselves). When the first array element (a[0]) is accessed, the CPU will see a load to virtual address 100. The hardware extracts the VPN from this (VPN=06), and uses that to check the TLB for a valid translation. Assuming this is the first time the program accesses the array, the result will be a TLB miss. The next access is to a[1], and there is some good news here: a TLB hit! Because the second element of the array is packed next to the first, it lives on the same page; because we’ve already accessed this page when accessing the first element of the array, the translation is already loaded ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES4 PAGING: FASTER TRANSLATIONS (TLBS) Offset 00 04 08 12 16 VPN = 00 VPN = 01 VPN = 02 VPN = 03 VPN = 04 VPN = 05 VPN = 06 VPN = 07 VPN = 08 VPN = 09 VPN = 10 VPN = 11 VPN = 12 VPN = 13 VPN = 14 VPN = 15 a[0] a[1] a[3] a[4] a[2] a[5] a[6] a[7] a[8] a[9] Figure 19.2: Example: An Array In A Tiny Address Space into the TLB. And hence the reason for our success. Access to a[2] encounters similar success (another hit), because it too lives on the same page as a[0] and a[1]. Unfortunately, when the program accesses a[3], we encounter another TLB miss. However, once again, the next entries (a[4] ... a[6]) will hit in the TLB, as they all reside on the same page in memory. Finally, access to a[7] causes one last TLB miss. The hardware once again consults the page table to figure out the location of this virtual page in physical memory, and updates the TLB accordingly. The final two accesses (a[8] and a[9]) receive the benefits of this TLB update; when the hardware looks in the TLB for their translations, two more hits result. Let us summarize TLB activity during our ten accesses to the array: miss, hit, hit, miss, hit, hit, hit, miss, hit, hit. Thus, our TLB hit rate, which is the number of hits divided by the total number of accesses, is 70%. Although this is not too high (indeed, we desire hit rates that approach 100%), it is non-zero, which may be a surprise. Even though this is the first time the program accesses the array, the TLB improves performance due to spatial locality. The elements of the array are packed tightly into pages (i.e., they are close to one another in space), and thus only the first access to an element on a page yields a TLB miss. Also note the role that page size plays in this example. If the page size OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]PAGING: FASTER TRANSLATIONS (TLBS) 5 TIP: USE CACHING WHEN POSSIBLE Caching is one of the most fundamental performance techniques in computer systems, one that is used again and again to make the “commoncase fast” [HP06]. The idea behind hardware caches is to take advantage of locality in instruction and data references. There are usually two types of locality: temporal locality and spatial locality. With temporal locality, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future. Think of loop variables or instructions in a loop; they are accessed repeatedly over time. With spatial locality, the idea is that if a program accesses memory at address x, it will likely soon access memory near x. Imagine here streaming through an array of some kind, accessing one element and then the next. Of course, these properties depend on the exact nature of the program, and thus are not hard-and-fast laws but more like rules of thumb. Hardware caches, whether for instructions, data, or address translations (as in our TLB)takeadvantageoflocality by keeping copies of memoryin small, fast on-chip memory. Instead of having to go to a (slow) memory to satisfy a request, the processor can first check if a nearby copy exists in a cache; if it does, the processor can access it quickly (i.e., in a few CPUcycles) andavoidspendingthecostlytimeittakes to access memory (many nanoseconds). You might be wondering: if caches (like the TLB) are so great, why don’t we just make bigger caches and keep all of our data in them? Unfortunately, this is where we run into more fundamental laws like those of physics. If you want a fast cache, it has to be small, as issues like the speed-of-light and other physical constraints become relevant. Any large cache by definition is slow, and thus defeats the purpose. Thus, we are stuck with small, fast caches; the question that remains is how to best use them to improve performance. had simply been twice as big (32 bytes, not 16), the array access would suffer even fewer misses. As typical page sizes are more like 4KB, these types of dense, array-based accesses achieve excellent TLB performance, encountering only a single miss per page of accesses. One last point about TLB performance: if the program, soon after this loop completes, accesses the array again, we’d likely see an even better result, assuming that we have a big enough TLB to cache the needed translations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit. In this case, the TLB hit rate would be high because of temporal locality, i.e., the quick re-referencing of memory items in time. Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties. If the program of interest exhibits such locality (and many programs do), the TLB hit rate will likely be high. ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES6 PAGING: FASTER TRANSLATIONS (TLBS) 1 2 3 4 5 6 7 8 9 10 11 VPN = (VirtualAddress & VPN_MASK) >> SHIFT (Success, TlbEntry) = TLB_Lookup(VPN) if (Success == True) // TLB Hit if (CanAccess(TlbEntry.ProtectBits) == True) Offset = VirtualAddress & OFFSET_MASK PhysAddr = (TlbEntry.PFN << SHIFT) | Offset Register = AccessMemory(PhysAddr) else RaiseException(PROTECTION_FAULT) else // TLB Miss RaiseException(TLB_MISS) Figure 19.3: TLB Control Flow Algorithm (OS Handled) 19.3 Who Handles The TLB Miss? Onequestion that we mustanswer: whohandlesaTLBmiss? Twoanswers are possible: the hardware, or the software (OS). In the olden days, the hardware had complex instruction sets (sometimes called CISC, for complex-instruction set computers) and the people who built the hardware didn’t much trust those sneaky OS people. Thus, the hardware would handle the TLB miss entirely. To do this, the hardware has to know exactly where the page tables are located in memory (via a pagetable base register, used in Line 11 in Figure 19.1), as well as their exact format; on a miss, the hardware would“walk”thepagetable, findthecorrect page-table entry and extract the desired translation, update the TLB with the translation, and retry the instruction. An example of an “older” architecture that has hardware-managed TLBs is the Intel x86 architecture, which uses a fixed multi-level page table (see the next chapter for details); the current page table is pointed to by the CR3 register [I09]. More modernarchitectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9 [WG00], both RISC or reduced-instruction set computers) have what is known as a software-managed TLB. On a TLB miss, the hardware simply raises an exception (line 11 in Figure 19.3), which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a trap handler. As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses. Whenrun, the code will lookup the translation in the page table, use special “privileged” instructions to update the TLB, and return from the trap; at this point, the hardware retries the instruction (resulting in a TLB hit). Let’s discuss a couple of important details. First, the return-from-trap instruction needs to be a little different than the return-from-trap we saw before when servicing a system call. In the latter case, the return-fromtrap should resume execution at the instruction after the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure. In the former case, when returning from a TLB miss-handling trap, the hardware must resume execution at the instruction that caused the trap; this retry thus lets the inOPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]PAGING: FASTER TRANSLATIONS (TLBS) 7 ASIDE: RISC VS. CISC In the 1980’s, a great battle took place in the computer architecture community. On one side was the CISC camp, which stood for Complex Instruction Set Computing; on the other side was RISC, for Reduced Instruction Set Computing [PS81]. The RISC side was spear-headed by DavidPatterson at Berkeley and John Hennessy at Stanford (who are also co-authors of some famousbooks[HP06]), althoughlater JohnCockewas recognized with a Turing award for his earliest work on RISC [CM00]. CISC instruction sets tend to have a lot of instructions in them, and each instruction is relatively powerful. For example, you might see a string copy, which takes two pointers and a length and copies bytes from source to destination. The idea behind CISC was that instructions should be high-level primitives, to make the assembly language itself easier to use, and to make code more compact. RISC instruction sets are exactly the opposite. A key observation behind RISC is that instruction sets are really compiler targets, and all compilers really want are a few simple primitives that they can use to generate high-performance code. Thus, RISC proponents argued, let’s rip out as much from the hardware as possible (especially the microcode), and make what’s left simple, uniform, and fast. In theearlydays, RISCchipsmadeahugeimpact,astheywerenoticeably faster [BC91]; many papers were written; a few companies were formed (e.g., MIPS and Sun). However, as time progressed, CISC manufacturers such as Intel incorporated many RISC techniques into the core of their processors, for example by adding early pipeline stages that transformed complex instructions into micro-instructions which could then be processed in a RISC-like manner. These innovations, plus a growing number of transistors on each chip, allowed CISC to remain competitive. The end result is that the debate died down, and today both types of processors can be made to run fast. struction run again, this time resulting in a TLB hit. Thus, depending on how a trap or exception was caused, the hardware must save a different PCwhentrappingintotheOS,inordertoresumeproperlywhenthetime to do so arrives. Second, whenrunningtheTLBmiss-handlingcode,theOSneedstobe extra careful not to cause an infinite chain of TLB misses to occur. Many solutions exist; for example, you could keep TLB miss handlers in physical memory (where they are unmapped and not subject to address translation), or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself; these wired translations always hit in the TLB. The primary advantage of the software-managed approach is flexibility: the OS can use any data structure it wants to implement the page ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES8 PAGING: FASTER TRANSLATIONS (TLBS) ASIDE: TLB VALID BIT ̸ = PAGE TABLE VALID BIT Acommonmistake is to confuse the valid bits found in a TLB with those found in a page table. In a page table, when a page-table entry (PTE) is marked invalid, it means that the page has not been allocated by the process, and should not beaccessed byacorrectly-working program. The usual response when an invalid page is accessed is to trap to the OS, which will respond by killing the process. ATLB valid bit, in contrast, simply refers to whether a TLB entry has a valid translation within it. When a system boots, for example, a common initial state for each TLB entry is to be set to invalid, because no address translations are yet cached there. Once virtual memory is enabled, and once programs start running and accessing their virtual address spaces, the TLB is slowly populated, and thus valid entries soon fill the TLB. The TLB valid bit is quite useful when performing a context switch too, as we’ll discuss further below. By setting all TLB entries to invalid, the system can ensure that the about-to-be-run process does not accidentally use a virtual-to-physical translation from a previous process. table, without necessitating hardware change. Another advantage is simplicity, as seen in the TLB control flow (line 11 in Figure 19.3, in contrast to lines 11–19 in Figure 19.1). The hardware doesn’t do much on a miss: just raise an exception and let the OS TLB miss handler do the rest. 19.4 TLB Contents: What’s In There? Let’s look at the contents of the hardwareTLBinmoredetail. Atypical TLB might have 32, 64, or 128 entries and be what is called fully associative. Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to f ind the desired translation. A TLB entry might look like this: VPN PFN otherbits Note that both the VPN and PFN are present in each entry, as a translation could end up in any of these locations (in hardware terms, the TLB is knownasafully-associative cache). The hardware searches the entries in parallel to see if there is a match. Moreinteresting are the “other bits”. For example, the TLB commonly has a valid bit, which says whether the entry has a valid translation or not. Also common are protection bits, which determine how a page can be accessed (as in the page table). For example, code pages might be marked read and execute, whereas heap pages might be marked read and write. There may also be a few other fields, including an address-space identifier, a dirty bit, and so forth; see below for more information. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]PAGING: FASTER TRANSLATIONS (TLBS) 9 19.5 TLB Issue: Context Switches With TLBs, new issues arise when switching between processes (and hence address spaces). Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from oneprocess to another, the hardware or OS(orboth)must becareful to ensurethat theabout-to-be-run process doesnotaccidentally use translations from some previously run process. To understand this situation better, let’s look at an example. When one process (P1) is running, it assumes the TLB might be caching translations that are valid for it, i.e., that come from P1’s page table. Assume, for this example, that the 10th virtual page of P1is mappedtophysicalframe100. In this example, assume another process (P2) exists, and the OS soon might decide to perform a context switch and run it. Assume here that the 10th virtual page of P2 is mapped to physical frame 170. If entries for both processes were in the TLB, the contents of the TLB would be: VPN PFN valid prot 10 100 1 rwx — — 0 — 10 170 1 rwx — — 0 — In the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can’t distinguish which entry is meant for which process. Thus, we need to do some more work in order for the TLB to correctly and efficiently support virtualization across multiple processes. And thus, a crux: THE CRUX: HOWTOMANAGETLBCONTENTS ON ACONTEXT SWITCH When context-switching between processes, the translations in the TLB for the last process are not meaningful to the about-to-be-run process. What should the hardware or OS do in order to solve this problem? There are a number of possible solutions to this problem. One approach is to simply flush the TLB on context switches, thus emptying it before running the next process. On a software-based system, this can be accomplished with an explicit (and privileged) hardware instruction; with a hardware-managed TLB,theflushcouldbeenactedwhenthe page-table base register is changed (note the OS must change the PTBR on a context switch anyhow). In either case, the flush operation simply sets all valid bits to 0, essentially clearing the contents of the TLB. By flushing the TLB on each context switch, we now have a working solution, as a process will never accidentally encounter the wrong trans©2008–23, ARPACI-DUSSEAU THREE EASY PIECES10 PAGING: FASTER TRANSLATIONS (TLBS) lations in the TLB. However, there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high. To reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches. In particular, some hardware systems provide an address space identifier (ASID) field in the TLB. You can think of the ASID as a process identifier (PID), but usually it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID). If we take our example TLB from above and add ASIDs, it is clear processes can readily share the TLB: only the ASID field is needed to differentiate otherwise identical translations. Here is a depiction of a TLB with the added ASID field: VPN PFN valid prot ASID 10 100 1 rwx 1 — — 0 — — 10 170 1 rwx 2 — — 0 — — Thus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion. Of course, the hardware also needs to know which process is currently running in order to perform translations, and thus the OS must, on a context switch, set some privileged register to the ASID of the current process. As an aside, you may also have thought of another case where two entries of the TLB are remarkably similar. In this example, there are two entries for two different processes with two different VPNs that point to the same physical page: VPN PFN valid prot ASID 10 101 1 r-x 1 — — 0 — — 50 101 1 r-x 2 — — 0 — — This situation might arise, for example, when two processes share a page (a code page, for example). In the example above, Process 1 is sharing physical page 101 with Process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of its address space. Sharing of code pages (in binaries, or shared libraries) is useful as it reduces the number of physical pages in use, thus reducing memoryoverheads. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]PAGING: FASTER TRANSLATIONS (TLBS) 11 19.6 Issue: Replacement Policy Aswithanycache, andthusalso with the TLB, one moreissue that we must consider is cache replacement. Specifically, when we are installing a new entry in the TLB, we have to replace an old one, and thus the question: which one to replace? THE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY Which TLB entry should be replaced when we add a new TLB entry? The goal, of course, being to minimize the miss rate (or increase hit rate) and thus improve performance. Wewillstudysuchpolicies in somedetail whenwetackletheproblem of swapping pages to disk; here we’ll just highlight a few typical policies. One common approach is to evict the least-recently-used or LRU entry. LRU tries to take advantage of locality in the memory-reference stream, assumingitislikely that an entrythat hasnotrecentlybeenusedisagood candidate for eviction. Another typical approach is to use a random policy, which evicts a TLB mapping at random. Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over n + 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better. 19.7 A Real TLB Entry Finally, let’s briefly look at a real TLB. This example is from the MIPS R4000[H93],amodernsystemthatusessoftware-managedTLBs;aslightly simplified MIPS TLB entry can be seen in Figure 19.4. TheMIPSR4000supportsa32-bitaddressspacewith4KBpages. Thus, we would expect a 20-bit VPN and 12-bit offset in our typical virtual address. However, as you can see in the TLB, there are only 19 bits for the VPN; as it turns out, user addresses will only come from half the address space (the rest reserved for the kernel) and hence only 19 bits of VPN are needed. The VPN translates to up to a 24-bit physical frame number (PFN), andhencecansupportsystemswithupto64GBof(physical)main memory(224 4KB pages). There are a few other interesting bits in the MIPS TLB. We see a global bit (G), which is used for pages that are globally-shared among processes. Thus, if the global bit is set, the ASID is ignored. We also see the 8-bit ASID, which the OS can use to distinguish between address spaces (as VPN G ASID PFN Figure 19.4: A MIPS TLB Entry ©2008–23, ARPACI-DUSSEAU C DV THREE EASY PIECES12 PAGING:FASTERTRANSLATIONS(TLBS) TIP:RAMISN’TALWAYSRAM(CULLER’SLAW) Thetermrandom-accessmemory,orRAM, impliesthatyoucanaccess anypartofRAMjustasquicklyasanother.Whileitisgenerallygoodto thinkofRAMinthisway,becauseofhardware/OSfeaturessuchasthe TLB,accessingaparticularpageofmemorymaybecostly,particularly ifthatpageisn’tcurrentlymappedbyyourTLB.Thus, itisalwaysgood toremember theimplementationtip: RAMisn’talwaysRAM.Sometimesrandomlyaccessingyouraddressspace,particularlyifthenumber ofpagesaccessedexceedstheTLBcoverage, canleadtosevereperformancepenalties. Becauseoneofouradvisors,DavidCuller,usedtoalwayspointtotheTLBasthesourceofmanyperformanceproblems,we namethislawinhishonor:Culler’sLaw. describedabove).Onequestionforyou:whatshouldtheOSdoifthere aremorethan256(28)processesrunningatatime? Finally,wesee3 Coherence(C)bits,whichdeterminehowapageiscachedbythehardware (abitbeyondthescopeofthesenotes);adirtybitwhichismarkedwhen thepagehasbeenwrittento(we’llseetheuseof thislater); avalidbit whichtellsthehardwareifthereisavalidtranslationpresentintheentry. Thereisalsoapagemaskfield(notshown),whichsupportsmultiplepage sizes;we’llseelaterwhyhavinglargerpagesmightbeuseful. Finally, someofthe64bitsareunused(shadedgrayinthediagram). MIPSTLBsusuallyhave32or64oftheseentries,mostofwhichare usedbyuserprocessesastheyrun.However,afewarereservedforthe OS.AwiredregistercanbesetbytheOStotellthehardwarehowmany slotsof theTLBtoreservefortheOS; theOSusesthesereservedmappingsforcodeanddatathatitwantstoaccessduringcriticaltimes,where aTLBmisswouldbeproblematic(e.g.,intheTLBmisshandler). BecausetheMIPSTLBissoftwaremanaged,thereneedstobeinstructionstoupdatetheTLB.TheMIPSprovidesfoursuchinstructions:TLBP, whichprobestheTLBtoseeifaparticulartranslationisinthere;TLBR, whichreadsthecontentsofaTLBentryintoregisters;TLBWI,whichreplacesaspecificTLBentry; andTLBWR,whichreplacesarandomTLB entry.TheOSusestheseinstructionstomanagetheTLB’scontents. Itis ofcoursecritical that theseinstructionsareprivileged; imaginewhata userprocesscoulddoifitcouldmodifythecontentsoftheTLB(hint: just aboutanything, includingtakeoverthemachine,runitsownmalicious “OS”,orevenmaketheSundisappear). 19.8Summary Wehaveseenhowhardwarecanhelpusmakeaddress translation faster.Byprovidingasmall,dedicatedon-chipTLBasanaddress-translation cache,mostmemoryreferenceswillhopefullybehandledwithouthaving toaccess thepagetable inmainmemory. Thus, inthecommoncase, OPERATING SYSTEMS [VERSION1.10] WWW.OSTEP.ORGPAGING: FASTER TRANSLATIONS (TLBS) 13 the performance of the program will be almost as if memory isn’t being virtualized at all, an excellent achievement for an operating system, and certainly essential to the use of paging in modern systems. However, TLBs do not make the world rosy for every program that exists. In particular, if the number of pages a program accesses in a short period of time exceeds the number of pages that fit into the TLB, the program will generate a large number of TLB misses, and thus run quite a bit more slowly. We refer to this phenomenon as exceeding the TLB coverage, and it can be quite a problem for certain programs. One solution, as we’ll discuss in the next chapter, is to include support for larger page sizes; by mapping key data structures into regions of the program’s address space that are mapped by larger pages, the effective coverage of the TLB can be increased. Support for large pages is often exploited by programs such as a database management system (a DBMS), which have certain data structures that are both large and randomly-accessed. One other TLB issue worth mentioning: TLB access can easily become abottleneck in the CPU pipeline, in particular with what is called a physically-indexed cache. With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. Because of this potential problem, people have looked into all sorts of clever ways to access caches with virtual addresses, thus avoiding the expensive step of translation in the case of a cache hit. Such a virtuallyindexed cache solves some performance problems, but introduces new issues into hardware design as well. See Wiggins’s fine survey for more details [W03].