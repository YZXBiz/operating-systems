22 Beyond Physical Memory: Policies In a virtual memory manager, life is easy when you have a lot of free memory. A page fault occurs, you find a free page on the free-page list, and assign it to the faulting page. Hey, Operating System, congratulations! You did it again. Unfortunately, things get a little more interesting when little memory is free. In such a case, this memory pressure forces the OS to start paging out pages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the replacement policy of the OS; historically, it was one of the most important decisions the early virtual memory systems made, as older systems had little physical memory. Minimally, it is an interesting set of policies worth knowing a little more about. And thus our problem: THE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT How can the OS decide which page (or pages) to evict from memory? This decision is madebythereplacementpolicyofthesystem, whichusually follows some general principles (discussed below) but also includes certain tweaks to avoid corner-case behaviors. 22.1 Cache Management Before diving into policies, we first describe the problem we are trying to solve in more detail. Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. Thus, our goal in picking a replacement policy for this cache is to minimize the number of cache misses, i.e., to minimize the number of times that we have to fetch a page from disk. Alternately, one can view our goal as maximizing the number of cache hits, i.e., the number of times a page that is accessed is found in memory. Knowing the number of cache hits and misses let us calculate the average memory access time (AMAT) for a program (a metric computer architects compute for hardware caches [HP06]). Specifically, given these values, we can compute the AMAT of a program as follows: AMAT = TM+(PMiss·TD) (22.1) 12 BEYOND PHYSICAL MEMORY: POLICIES where TM represents the cost of accessing memory, TD the cost of accessing disk, and PMiss the probability of not finding the data in the cache (a miss); PMiss varies from 0.0 to 1.0, and sometimes we refer to a percent miss rate instead of a probability (e.g., a 10% miss rate means PMiss = 0.10). Note you always pay the cost of accessing the data in memory; whenyoumiss, however, you must additionally pay the cost of fetching the data from disk. For example, let us imagine a machine with a (tiny) address space: 4KB, with 256-byte pages. Thus, a virtual address has two components: a 4-bit VPN(themost-significantbits)andan8-bitoffset(theleast-significant bits). Thus, a process in this example can access 24 or 16 total virtual pages. In this example, the process generates the following memory references (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x300, 0x400, 0x500, 0x600, 0x700, 0x800, 0x900. These virtual addresses refer to the first byte of each of the first ten pages of the address space (the page number being the first hex digit of each virtual address). Let us further assume that every page except virtual page 3 is already in memory. Thus, our sequence of memory references will encounter the following behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit, hit. We can computethehitrate (the percent of references found in memory): 90%, as 9 out of 10 references are in memory. The miss rate is thus 10% (PMiss = 0.1). In general, PHit + PMiss = 1.0; hit rate plus miss rate sum to 100%. To calculate AMAT, we need to know the cost of accessing memory and the cost of accessing disk. Assuming the cost of accessing memory (TM) is around 100 nanoseconds, and the cost of accessing disk (TD) is about 10 milliseconds, we have the following AMAT: 100ns+0.1·10ms, which is 100ns + 1ms, or 1.0001 ms, or about 1 millisecond. If our hit rate had instead been 99.9% (Pmiss = 0.001), the result is quite different: AMAT is 10.1 microseconds, or roughly 100 times faster. As the hit rate approaches 100%, AMAT approaches 100 nanoseconds. Unfortunately, as you can see in this example, the cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs. Clearly, we need to avoid as manymisses as possible or run slowly, at the rate of the disk. One way to help with this is to carefully develop a smart policy, as we now do. 22.2 The Optimal Replacement Policy To better understand how a particular replacement policy works, it would be nice to compare it to the best possible replacement policy. As it turns out, such an optimal policy was developed by Belady many years ago [B66] (he originally called it MIN). The optimal replacement policy leads to the fewest number of misses overall. Belady showed that a simple (but, unfortunately, difficult to implement!) approach that replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]BEYOND PHYSICAL MEMORY: POLICIES 3 TIP: COMPARING AGAINST OPTIMAL IS USEFUL Although optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies. Saying that your fancy newalgorithm has a80%hitrate isn’t meaningful in isolation; saying that optimalachievesan82%hitrate(andthusyournewapproach is quite close to optimal) makes the result more meaningful and gives it context. Thus, in any study you perform, knowing what the optimal is lets you perform a better comparison, showing how much improvement is still possible, and also when you can stop making your policy better, because it is close enough to the ideal [AD03]. Hopefully, the intuition behind the optimal policy makes sense. Think about it like this: if you have to throw out some page, why not throw out the one that is needed the furthest from now? By doing so, you are essentially saying that all the other pages in the cache are more important than the one furthest out. The reason this is true is simple: you will refer to the other pages before you refer to the one furthest out. Let’s trace through a simple example to understand the decisions the optimal policy makes. Assume a program accesses the following stream of virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Figure 22.1 shows the behavior of optimal, assuming a cache that fits three pages. In the figure, you can see the following actions. Not surprisingly, the f irst three accesses are misses, as the cache begins in an empty state; such a missis sometimesreferred to as a cold-start miss (or compulsory miss). Then werefer again to pages 0 and 1, which both hit in the cache. Finally, we reach another miss (to page 3), but this time the cache is full; a replacement must take place! Which begs the question: which page should wereplace? With the optimal policy, we examine the future for each page currently in the cache (0, 1, and 2), and see that0isaccessed almostimmediately, 1 is accessed a little later, and 2 is accessed furthest in the future. Thus the optimal policy has an easy choice: evict page 2, resulting in Resulting Access Hit/Miss? Evict CacheState 0 1 2 0 1 3 0 3 1 2 1 Miss Miss Miss Hit Hit Miss Hit Hit Hit Miss Hit Figure 22.1: Tracing The Optimal Policy ©2008–23, ARPACI-DUSSEAU 0 0, 1 0, 1, 2 0, 1, 2 0, 1, 2 2 3 0, 1, 3 0, 1, 3 0, 1, 3 0, 1, 3 0, 1, 2 0, 1, 2 THREE EASY PIECES4 BEYONDPHYSICALMEMORY:POLICIES ASIDE:TYPESOFCACHEMISSES Inthecomputerarchitectureworld, architectssometimesfindituseful tocharacterizemissesbytype, intooneofthreecategories: compulsory, capacity, andconflictmisses, sometimescalledtheThreeC’s[H87]. A compulsorymiss(orcold-startmiss[EF78])occursbecausethecacheis emptytobeginwithandthis is thefirst referencetotheitem; incontrast,acapacitymissoccursbecausethecacheranoutofspaceandhad toevictanitemtobringanewitemintothecache. Thethirdtypeof miss(aconflictmiss)arisesinhardwarebecauseof limitsonwherean itemcanbeplacedinahardwarecache,duetosomethingknownassetassociativity; itdoesnotariseintheOSpagecachebecausesuchcaches arealwaysfully-associative, i.e., therearenorestrictionsonwhere in memoryapagecanbeplaced.SeeH&Pfordetails[HP06]. pages0,1,and3inthecache.Thenextthreereferencesarehits,butthen weget topage2,whichweevictedlongago, andsufferanothermiss. Heretheoptimalpolicyagainexaminesthefutureforeachpageinthe cache(0,1,and3),andseesthataslongasitdoesn’tevictpage1(which isabouttobeaccessed),we’llbeOK.Theexampleshowspage3getting evicted,although0wouldhavebeenafinechoicetoo.Finally,wehiton page1andthetracecompletes. Wecanalsocalculatethehitrateforthecache:with6hitsand5misses, thehitrateis Hits Hits+Misses whichis 6 6+5 or54.5%.Youcanalsocompute thehitratemodulocompulsorymisses(i.e.,ignorethefirstmisstoagiven page),resultingina85.7%hitrate. Unfortunately, aswesawbefore inthedevelopmentof scheduling policies, thefutureisnotgenerallyknown;youcan’tbuildtheoptimal policyforageneral-purposeoperatingsystem1. Thus, indevelopinga real,deployablepolicy,wewillfocusonapproachesthatfindsomeother waytodecidewhichpagetoevict. Theoptimalpolicywill thusserve onlyasacomparisonpoint,toknowhowcloseweareto“perfect”. 22.3ASimplePolicy:FIFO Manyearlysystemsavoidedthecomplexityof tryingtoapproach optimalandemployedverysimplereplacementpolicies. Forexample, somesystemsusedFIFO(first-in, first-out) replacement,wherepages weresimplyplacedinaqueuewhentheyenterthesystem;whenareplacementoccurs,thepageonthetailofthequeue(the“first-in”page)is evicted.FIFOhasonegreatstrength: itisquitesimpletoimplement. Let’sexaminehowFIFOdoesonourexamplereferencestream(Figure 22.2,page5).Weagainbeginourtracewiththreecompulsorymissesto 1Ifyoucan,letusknow!Wecanbecomerichtogether.Or,likethescientistswho“discovered”coldfusion,widelyscornedandmocked[FP89]. OPERATING SYSTEMS [VERSION1.10] WWW.OSTEP.ORGBEYOND PHYSICAL MEMORY: POLICIES 5 Access Hit/Miss? Evict Resulting Cache State 0 1 2 0 1 3 0 3 1 2 1 Miss Miss Miss Hit Hit Miss Miss Hit Miss Miss Hit First-in→ First-in→ 0 0, 1 First-in→ 0, 1, 2 First-in→ 0, 1, 2 First-in→ 0, 1, 2 0 1 2 3 First-in→ 1, 2, 3 First-in→ 2, 3, 0 First-in→ 2, 3, 0 First-in→ 3, 0, 1 First-in→ 0, 1, 2 First-in→ 0, 1, 2 Figure 22.2: Tracing The FIFO Policy pages 0, 1, and 2, and then hit on both 0 and 1. Next, page 3 is referenced, causing a miss; the replacement decision is easy with FIFO: pick the page that was the “first one” in (the cache state in the figure is kept in FIFO order, with the first-in page on the left), which is page 0. Unfortunately, our next access is to page 0, causing another miss and replacement (of page 1). We then hit on page 3, but miss on 1 and 2, and finally hit on 1. Comparing FIFO to optimal, FIFO does notably worse: a 36.4% hit rate (or 57.1% excluding compulsory misses). FIFO simply can’t determine the importance of blocks: even though page 0 had been accessed a number of times, FIFO still kicks it out, simply because it was the first one brought into memory. ASIDE: BELADY’S ANOMALY Belady (of the optimal policy) and colleagues found an interesting reference stream that behaved a little unexpectedly [BNS69]. The memoryreference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacement policy they were studying was FIFO. The interesting part: how the cache hit rate changed when moving from a cache size of 3 to 4 pages. In general, you would expect the cache hit rate to increase (get better) whenthecache gets larger. But in this case, with FIFO, it gets worse! Calculate the hits andmissesyourselfandsee. Thisoddbehaviorisgenerally referred to as Belady’s Anomaly (to the chagrin of his co-authors). Some other policies, such as LRU, don’t suffer from this problem. Can you guess why? As it turns out, LRU has what is known as a stack property [M+70]. For algorithms with this property, a cache of size N + 1 naturally includes the contents of a cache of size N. Thus, when increasing the cache size, hit rate will either stay the same or improve. FIFO and Random (among others) clearly do not obey the stack property, and thus are susceptible to anomalous behavior. ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES6 BEYONDPHYSICALMEMORY:POLICIES Resulting Access Hit/Miss? Evict CacheState 0 Miss 0 1 Miss 0,1 2 Miss 0,1,2 0 Hit 0,1,2 1 Hit 0,1,2 3 Miss 0 1,2,3 0 Miss 1 2,3,0 3 Hit 2,3,0 1 Miss 3 2,0,1 2 Hit 2,0,1 1 Hit 2,0,1 Figure22.3:TracingTheRandomPolicy 22.4AnotherSimplePolicy:Random AnothersimilarreplacementpolicyisRandom,whichsimplypicksa randompagetoreplaceundermemorypressure.Randomhasproperties similartoFIFO; it issimpletoimplement,but itdoesn’treallytrytobe toointelligentinpickingwhichblockstoevict.Let’slookathowRandom doesonourfamousexamplereferencestream(seeFigure22.3). Ofcourse,howRandomdoesdependsentirelyuponhowlucky(or unlucky)Randomgetsinitschoices. Intheexampleabove,Randomdoes alittlebetterthanFIFO,andalittleworsethanoptimal. Infact,wecan runtheRandomexperiment thousandsof timesanddeterminehowit doesingeneral.Figure22.4showshowmanyhitsRandomachievesover 10,000trials,eachwithadifferentrandomseed. Asyoucansee, sometimes(justover40%ofthetime),Randomisasgoodasoptimal,achieving 6hitsontheexampletrace;sometimesitdoesmuchworse,achieving2 hitsorfewer.HowRandomdoesdependsontheluckofthedraw. 0 1 2 3 4 5 6 7 0 10 20 30 40 50 Number of Hits Frequency Figure22.4:RandomPerformanceOver10,000Trials OPERATING SYSTEMS [VERSION1.10] WWW.OSTEP.ORGBEYOND PHYSICAL MEMORY: POLICIES 7 Access Hit/Miss? Evict Resulting Cache State 0 1 2 0 1 3 0 3 1 2 1 22.5 Using History: LRU Miss Miss Miss Hit Hit Miss Hit Hit Hit Miss Hit LRU→ LRU→ 0 0, 1 LRU→ 0,1,2 LRU→ 1,2,0 LRU→ 2,0,1 2 0 LRU→ 0,1,3 LRU→ 1,3,0 LRU→ 1,0,3 LRU→ 0,3,1 LRU→ 3,1,2 LRU→ 3,2,1 Figure 22.5: Tracing The LRU Policy Unfortunately, any policy as simple as FIFO or Random is likely to have a common problem: it might kick out an important page, one that is about to be referenced again. FIFO kicks out the page that was first brought in; if this happens to be a page with important code or data structures upon it, it gets thrown out anyhow, even though it will soon be paged back in. Thus, FIFO, Random, and similar policies are not likely to approach optimal; something smarter is needed. As wedid with scheduling policy, to improve our guess at the future, weonceagain lean on the past and use history as our guide. For example, if a program has accessed a page in the near past, it is likely to access it again in the near future. One type of historical information a page-replacement policy could use is frequency; if a page has been accessed many times, perhaps it should not be replaced as it clearly has some value. A more commonlyused property of a page is its recency of access; the more recently a page has been accessed, perhaps the more likely it will be accessed again. This family of policies is based on what people refer to as the principle of locality [D70], which basically is just an observation about programs and their behavior. What this principle says, quite simply, is that programs tend to access certain code sequences (e.g., in a loop) and data structures (e.g., an array accessed bytheloop)quitefrequently; weshould thus try to use history to figure out which pages are important, and keep those pages in memory when it comes to eviction time. And thus, a family of simple historically-based algorithms are born. The Least-Frequently-Used (LFU) policy replaces the least-frequentlyusedpagewhenanevictionmusttakeplace. Similarly,theLeast-RecentlyUsed (LRU) policy replaces the least-recently-used page. These algorithms are easy toremember: onceyouknowthename,youknowexactly what it does, which is an excellent property for a name. To better understand LRU, let’s examine how LRU does on our exam©2008–23, ARPACI-DUSSEAU THREE EASY PIECES8 BEYOND PHYSICAL MEMORY: POLICIES ASIDE: TYPES OF LOCALITY There are two types of locality that programs tend to exhibit. The first is known as spatial locality, which states that if a page P is accessed, it is likely the pages around it (say P − 1 or P + 1) will also likely be accessed. The second is temporal locality, which states that pages that have been accessed in the near past are likely to be accessed again in the near future. The assumption of the presence of these types of locality plays a large role in the caching hierarchies of hardware systems, which deploy many levels of instruction, data, and address-translation caching to help programs run fast when such locality exists. Of course, the principle of locality, as it is often called, is no hard-andfast rule that all programs must obey. Indeed, some programs access memory (or disk) in rather random fashion and don’t exhibit much or any locality in their access streams. Thus, while locality is a good thing to keep in mindwhile designing caches of any kind (hardware or software), it does not guarantee success. Rather, it is a heuristic that often proves useful in the design of computer systems. ple reference stream. Figure 22.5 (page 7) shows the results. From the f igure, you can see how LRU can use history to do better than stateless policies such as RandomorFIFO.Intheexample,LRUevictspage2when it first has to replace a page, because 0 and 1 have been accessed more recently. It then replaces page 0 because 1 and 3 have been accessed more recently. In both cases, LRU’s decision, based on history, turns out to be correct, and the next references are thus hits. Thus, in our example, LRU does as well as possible, matching optimal in its performance2. Weshouldalsonotethattheoppositesofthese algorithmsexist: MostFrequently-Used (MFU) and Most-Recently-Used (MRU). In most cases (not all!), these policies do not work well, as they ignore the locality most programs exhibit instead of embracing it. 22.6 Workload Examples Let’s look at a few more examples in order to better understand how some of these policies behave. Here, we’ll examine more complex workloads instead of small traces. However, even these workloads are greatly simplified; a better study would include application traces. Our first workload has no locality, which means that each reference is to a random page within the set of accessed pages. In this simple example, the workload accesses 100 unique pages over time, choosing the next page to refer to at random; overall, 10,000 pages are accessed. In the experiment, we vary the cache size from very small (1 page) to enough to hold all the unique pages (100 pages), in order to see how each policy behaves over the range of cache sizes. 2OK, wecooked the results. But sometimes cooking is necessary to prove a point. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]BEYOND PHYSICAL MEMORY: POLICIES 9 The No-Locality Workload 100% 80% Hit Rate 60% 40% 20% 0% OPT LRU FIFO RAND 0 20 40 60 Cache Size (Blocks) 80 100 Figure 22.6: The No-Locality Workload Figure 22.6 plots the results of the experiment for optimal, LRU, Random,andFIFO.They-axisofthefigureshowsthehitratethateachpolicy achieves; the x-axis varies the cache size as described above. We can draw a number of conclusions from the graph. First, when there is no locality in the workload, it doesn’t matter much which realistic policy you are using; LRU, FIFO, and Randomallperformthesame, with the hit rate exactly determined by the size of the cache. Second, when the cache is large enough to fit the entire workload, it also doesn’t matter which policy you use; all policies (even Random) converge to a 100% hit rate when all the referenced blocks fit in cache. Finally, you can see that optimalperformsnoticeablybetterthantherealistic policies; peeking into the future, if it were possible, does a much better job of replacement. The next workload we examine is called the “80-20” workload, which exhibits locality: 80% of the references are made to 20% of the pages (the “hot” pages); the remaining 20% of the references are made to the remaining 80% of the pages (the “cold” pages). In our workload, there are a total 100 unique pages again; thus, “hot” pages are referred to most of the time, and “cold” pages the remainder. Figure 22.7 (page 10) shows howthe policies perform with this workload. As you can see from the figure, while both random and FIFO do reasonably well, LRU does better, as it is more likely to hold onto the hot pages; as those pages have been referred to frequently in the past, they are likely to be referred to again in the near future. Optimal once again does better, showing that LRU’s historical information is not perfect. ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES10 BEYOND PHYSICAL MEMORY: POLICIES The 80-20 Workload 100% 80% Hit Rate 60% 40% 20% 0% 0 20 40 60 Cache Size (Blocks) OPT LRU FIFO RAND 80 Figure 22.7: The 80-20 Workload 100 You might now be wondering: is LRU’s improvement over Random andFIFOreallythatbigofadeal? Theanswer,asusual,is“itdepends.” If each missis verycostly (not uncommon), thenevenasmallincreaseinhit rate (reduction in miss rate) can make a huge difference on performance. If misses are not so costly, then of course the benefits possible with LRU are not nearly as important. Let’s look at one final workload. We call this one the “looping sequential” workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, ..., up to page 49, and then we loop, repeating those accesses, for a total of 10,000 accesses to 50 unique pages. The last graph in Figure 22.8 shows the behavior of the policies under this workload. This workload, common in many applications (including important commercial applications such as databases [CD85]), represents a worstcase for bothLRUandFIFO.Thesealgorithms,underalooping-sequential workload, kick out older pages; unfortunately, due to the looping nature of the workload, these older pages are going to be accessed sooner than the pages that the policies prefer to keep in cache. Indeed, even with a cache of size 49, a looping-sequential workload of 50 pages results in a 0% hit rate. Interestingly, Random fares notably better, not quite approaching optimal, but at least achieving a non-zero hit rate. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]BEYOND PHYSICAL MEMORY: POLICIES 11 The Looping-Sequential Workload 100% 80% Hit Rate 60% 40% 20% 0% OPT LRU FIFO RAND 0 20 40 60 Cache Size (Blocks) 80 Figure 22.8: The Looping Workload 22.7 Implementing Historical Algorithms 100 As you can see, an algorithm such as LRU can generally do a better job than simpler policies like FIFO or Random, which may throw out important pages. Unfortunately, historical policies present us with a new challenge: how do we implement them? Let’s take, for example, LRU. To implement it perfectly, we need to do a lot of work. Specifically, upon each page access (i.e., each memory access, whether an instruction fetch or a load or store), we must update some data structure to move this page to the front of the list (i.e., the MRU side). Contrast this to FIFO, where the FIFO list of pages is only accessed when a page is evicted (by removing the first-in page) or when a new page is added to the list (to the last-in side). To keep track of which pages havebeenleast- andmost-recently used, the system has to dosome accounting work on every memory reference. Clearly, without great care, such accounting could greatly reduce performance. Onemethodthat could help speed this up is to add a little bit of hardwaresupport. Forexample,amachinecouldupdate,oneachpageaccess, a timefieldinmemory(forexample, thiscouldbeintheper-process page table, or just in some separate array in memory, with one entry per physical page of the system). Thus, when a page is accessed, the time field would be set, by hardware, to the current time. Then, when replacing a page, the OScouldsimplyscan all the time fields in the system to findthe least-recently-used page. ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES12 BEYOND PHYSICAL MEMORY: POLICIES Unfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive. Imagine a modern machine with 4GB of memory, chopped into 4KB pages. This machine has 1 million pages, and thus f inding the LRU page will take a long time, even at modern CPU speeds. Which begs the question: do we really need to find the absolute oldest page to replace? Can we instead survive with an approximation? CRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY Given that it will be expensive to implement perfect LRU, can we approximate it in some way, and still obtain the desired behavior? 22.8 Approximating LRU As it turns out, the answer is yes: approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a use bit (sometimes called the reference bit), the first of which was implemented in the first system with paging, the Atlas onelevel store [KE+62]. There is one use bit per page of the system, and the usebits live in memorysomewhere(theycouldbeintheper-processpage tables, for example, or just in an array somewhere). Whenever a page is referenced (i.e., read or written), the use bit is set by hardware to 1. The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS. HowdoestheOSemploytheusebit to approximate LRU? Well, there could be a lot of ways, but with the clock algorithm [C69], one simple approach was suggested. Imagine all the pages of the system arranged in a circular list. A clock hand points to some particular page to begin with (it doesn’t really matter which). When a replacement must occur, the OS checks if the currently-pointed to page P has a use bit of 1 or 0. If 1, this implies that page P was recently used and thus is not a good candidate for replacement. Thus, the use bit for P is set to 0 (cleared), and the clock hand is incremented to the next page (P + 1). The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used (or, in the worst case, that all pages have been and that we have now searched through the entire set of pages, clearing all the bits). Note that this approach is not the only way to employ a use bit to approximate LRU. Indeed, any approach which periodically clears the use bits and then differentiates between which pages have use bits of 1 versus 0 to decide which to replace would be fine. The clock algorithm of Corbato’s was just one early approach which met with some success, and had the nice property of not repeatedly scanning through all of memory looking for an unused page. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]BEYOND PHYSICAL MEMORY: POLICIES 13 The 80-20 Workload 100% 80% Hit Rate 60% 40% 20% 0% 0 20 40 60 Cache Size (Blocks) OPT LRU FIFO RAND Clock 80 100 Figure 22.9: The 80-20 Workload With Clock Thebehavior of aclock algorithm variant is shown in Figure 22.9. This variant randomly scans pages when doing a replacement; when it encounters a page with a reference bit set to 1, it clears the bit (i.e., sets it to 0); when it finds a page with the reference bit set to 0, it chooses it as its victim. As you can see, although it doesn’t do quite as well as perfect LRU, it does better than approaches that don’t consider history at all. 22.9 Considering Dirty Pages One small modification to the clock algorithm (also originally suggested by Corbato [C69]) that is commonly made is the additional consideration of whether a page has been modified or not while in memory. The reason for this: if a page has been modified and is thus dirty, it must be written back to disk to evict it, which is expensive. If it has not been modified (and is thus clean), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O. Thus, some VMsystems prefer to evict clean pages over dirty pages. To support this behavior, the hardware should include a modified bit (a.k.a. dirty bit). This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm. The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; failing to find those, then for unused pages that are dirty, and so forth. ©2008–23, ARPACI-DUSSEAU THREE EASY PIECES14 BEYOND PHYSICAL MEMORY: POLICIES 22.10 Other VM Policies Page replacement is not the only policy the VM subsystem employs (though it may be the most important). For example, the OS also has to decide when to bring a page into memory. This policy, sometimes called the page selection policy (as it was called by Denning [D70]), presents the OS with some different options. For most pages, the OS simply uses demandpaging, whichmeansthe OS brings the page into memory when it is accessed, “on demand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as prefetching and should only be done when there is reasonable chance of success. For example, some systems will assume that if a code page P is brought into memory,thatcodepageP+1willlikelysoonbeaccessedandthusshould be brought into memory too. Another policy determines how the OS writes pages out to disk. Of course, they could simply be written out one at a time; however, many systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write. This behavior is usually called clustering or simply grouping of writes, and is effective because of the nature of disk drives, which perform a single large write more efficiently than many small ones. 22.11 Thrashing Before closing, we address one final question: what should the OS do whenmemoryissimplyoversubscribed,andthememorydemandsofthe set of running processes simply exceeds the available physical memory? In this case, the system will constantly be paging, a condition sometimes referred to as thrashing [D70]. Some earlier operating systems had a fairly sophisticated set of mechanisms to both detect and cope with thrashing when it took place. For example, given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes’ working sets (the pages that they are using actively) fit in memory and thus can make progress. This approach, generally known as admission control, states that it is sometimes better to do less work well than to try to do everything at once poorly, a situation we often encounter in real life as well as in modern computer systems (sadly). Some current systems take more a draconian approach to memory overload. For example, some versions of Linux run an out-of-memory killer when memory is oversubscribed; this daemon chooses a memoryintensive process and kills it, thus reducing memory in a none-too-subtle manner. While successful at reducing memory pressure, this approach can have problems, if, for example, it kills the X server and thus renders any applications requiring the display unusable. OPERATING SYSTEMS WWW.OSTEP.ORG [VERSION 1.10]BEYOND PHYSICAL MEMORY: POLICIES 15 22.12 Summary Wehave seen the introduction of a number of page-replacement (and other) policies, which are part of the VM subsystem of all modern operating systems. Modern systems add some tweaks to straightforward LRU approximations like clock; for example, scan resistance is an important part of manymodernalgorithms,suchasARC[MM03]. Scan-resistantalgorithms are usually LRU-like but also try to avoid the worst-case behavior of LRU, which we saw with the looping-sequential workload. Thus, the evolution of page-replacement algorithms continues. For many years, the importance of replacement algorithms had decreased, as thediscrepancybetweenmemory-accessanddisk-accesstimes was so large. Specifically, because paging to disk was so expensive, the cost of frequent paging was prohibitive; simply put, no matter how good your replacement algorithm was, if you were performing frequent replacements, your system became unbearably slow. Thus, the best solution was a simple (if intellectually unsatisfying) one: buy more memory. However,recentinnovationsinmuchfasterstoragedevices(e.g., Flashbased SSDs) have changed these performance ratios yet again, leading to a renaissance in page replacement algorithms. See [SS10,W+21] for recent work in this space.